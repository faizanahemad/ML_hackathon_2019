{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T10:56:06.982560Z",
     "start_time": "2019-06-12T10:56:06.943268Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np_utils\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, DepthwiseConv2D, Conv2D, SeparableConv2D, MaxPooling1D\n",
    "from keras.layers import Input, concatenate\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Nadam, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.regularizers import l2\n",
    "%config InlineBackend.figure_format='retina'\n",
    "from keras_contrib.callbacks import CyclicLR\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from data_science_utils.vision.keras import *\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import missingno as msno\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "from data_science_utils import dataframe as df_utils\n",
    "from data_science_utils import models as model_utils\n",
    "from data_science_utils import plots as plot_utils\n",
    "from data_science_utils.dataframe import column as column_utils\n",
    "from data_science_utils import misc as misc\n",
    "from data_science_utils import preprocessing as pp_utils\n",
    "from data_science_utils import nlp as nlp_utils\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from data_science_utils.dataframe import get_specific_cols\n",
    "\n",
    "import more_itertools\n",
    "from more_itertools import flatten\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lib' from '/home/ec2-user/SageMaker/ML_hackathon_2019/lib.py'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "from importlib import reload\n",
    "import lib\n",
    "reload(lib)\n",
    "from lib import *\n",
    "\n",
    "from oclr import OneCycleLR, LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T09:42:02.848654Z",
     "start_time": "2019-06-12T09:41:52.143642Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"price_prediction/train.csv\")\n",
    "df_test = pd.read_csv(\"price_prediction/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9160b7525314b08bc7320826983ac47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1449608), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c54bfb31724669a205e6196b104b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1449608), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b2025dac20405baa85083348a8941a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1449608), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0231b20ef61e4e3694b1351019337158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=362403), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7c846b73ef4696a5db953dfca4921c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=362403), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737a1656e90e4c79a6540c48ea7524d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=362403), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_train['text'] = Parallel(n_jobs=20, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_train['text']))\n",
    "df_train['text_encoded'] = Parallel(n_jobs=20, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_train['text_encoded']))\n",
    "df_train['char_encoded'] = Parallel(n_jobs=20, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_train['char_encoded']))\n",
    "\n",
    "df_test['text'] = Parallel(n_jobs=20, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_test['text']))\n",
    "df_test['text_encoded'] = Parallel(n_jobs=20, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_test['text_encoded']))\n",
    "df_test['char_encoded'] = Parallel(n_jobs=20, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_test['char_encoded']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T10:40:19.816205Z",
     "start_time": "2019-06-12T10:39:01.196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>GL</th>\n",
       "      <th>text</th>\n",
       "      <th>text_encoded</th>\n",
       "      <th>char</th>\n",
       "      <th>char_encoded</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1489960</td>\n",
       "      <td>gl_jewelry</td>\n",
       "      <td>['livsmart', 'resin', 'jewellery', 'display', ...</td>\n",
       "      <td>[28683, 1558, 273, 310, 196, 8, 4, 31, 453, 33...</td>\n",
       "      <td>Livsmart Resin Jewellery Display Stand, 17x11c...</td>\n",
       "      <td>[38, 8, 28, 10, 19, 4, 7, 6, 2, 45, 3, 10, 8, ...</td>\n",
       "      <td>507.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>633493</td>\n",
       "      <td>gl_digital_ebook_purchase</td>\n",
       "      <td>['quantum', 'creation', 'supernatural', 'lurk'...</td>\n",
       "      <td>[7567, 879, 12991, 1, 3900, 207, 267]</td>\n",
       "      <td>Quantum Creation: Does the Supernatural Lurk i...</td>\n",
       "      <td>[78, 15, 4, 9, 6, 15, 19, 2, 23, 7, 3, 4, 6, 8...</td>\n",
       "      <td>479.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1474591</td>\n",
       "      <td>gl_apparel</td>\n",
       "      <td>['izod', 'men', 'casual', 'shirt', '_NUM30_', ...</td>\n",
       "      <td>[5244, 24, 35, 32, 127, 26671, 64, 5368, 9, 90...</td>\n",
       "      <td>IZOD Men's Casual Shirt (8907163477392_ZKSH019...</td>\n",
       "      <td>[41, 81, 46, 39, 2, 29, 3, 9, 63, 10, 2, 23, 4...</td>\n",
       "      <td>829.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>830218</td>\n",
       "      <td>gl_apparel</td>\n",
       "      <td>['dishaa', 'woman', 'rayon', 'line', 'kurta', ...</td>\n",
       "      <td>[45999, 21, 572, 192, 222, 31, 133, 2162, 369,...</td>\n",
       "      <td>Dishaa Women's Rayon A-Line Kurta (Black, X-La...</td>\n",
       "      <td>[39, 8, 10, 13, 4, 4, 2, 42, 5, 19, 3, 9, 63, ...</td>\n",
       "      <td>648.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201144</td>\n",
       "      <td>gl_digital_ebook_purchase</td>\n",
       "      <td>['return', 'raven', 'ulfrik', 'ormsson', 'saga...</td>\n",
       "      <td>[854, 13891, 1, 1, 7870, 597, 2, 267]</td>\n",
       "      <td>Return of the Ravens (Ulfrik Ormsson's Saga Bo...</td>\n",
       "      <td>[45, 3, 6, 15, 7, 9, 2, 5, 21, 2, 6, 13, 3, 2,...</td>\n",
       "      <td>332.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                         GL  \\\n",
       "0  1489960                 gl_jewelry   \n",
       "1   633493  gl_digital_ebook_purchase   \n",
       "2  1474591                 gl_apparel   \n",
       "3   830218                 gl_apparel   \n",
       "4   201144  gl_digital_ebook_purchase   \n",
       "\n",
       "                                                text  \\\n",
       "0  ['livsmart', 'resin', 'jewellery', 'display', ...   \n",
       "1  ['quantum', 'creation', 'supernatural', 'lurk'...   \n",
       "2  ['izod', 'men', 'casual', 'shirt', '_NUM30_', ...   \n",
       "3  ['dishaa', 'woman', 'rayon', 'line', 'kurta', ...   \n",
       "4  ['return', 'raven', 'ulfrik', 'ormsson', 'saga...   \n",
       "\n",
       "                                        text_encoded  \\\n",
       "0  [28683, 1558, 273, 310, 196, 8, 4, 31, 453, 33...   \n",
       "1              [7567, 879, 12991, 1, 3900, 207, 267]   \n",
       "2  [5244, 24, 35, 32, 127, 26671, 64, 5368, 9, 90...   \n",
       "3  [45999, 21, 572, 192, 222, 31, 133, 2162, 369,...   \n",
       "4              [854, 13891, 1, 1, 7870, 597, 2, 267]   \n",
       "\n",
       "                                                char  \\\n",
       "0  Livsmart Resin Jewellery Display Stand, 17x11c...   \n",
       "1  Quantum Creation: Does the Supernatural Lurk i...   \n",
       "2  IZOD Men's Casual Shirt (8907163477392_ZKSH019...   \n",
       "3  Dishaa Women's Rayon A-Line Kurta (Black, X-La...   \n",
       "4  Return of the Ravens (Ulfrik Ormsson's Saga Bo...   \n",
       "\n",
       "                                        char_encoded   PRICE  \n",
       "0  [38, 8, 28, 10, 19, 4, 7, 6, 2, 45, 3, 10, 8, ...  507.62  \n",
       "1  [78, 15, 4, 9, 6, 15, 19, 2, 23, 7, 3, 4, 6, 8...  479.90  \n",
       "2  [41, 81, 46, 39, 2, 29, 3, 9, 63, 10, 2, 23, 4...  829.28  \n",
       "3  [39, 8, 10, 13, 4, 4, 2, 42, 5, 19, 3, 9, 63, ...  648.31  \n",
       "4  [45, 3, 6, 15, 7, 9, 2, 5, 21, 2, 6, 13, 3, 2,...  332.00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>GL</th>\n",
       "      <th>text</th>\n",
       "      <th>text_encoded</th>\n",
       "      <th>char</th>\n",
       "      <th>char_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1585751</td>\n",
       "      <td>gl_apparel</td>\n",
       "      <td>['folklore', 'woman', 'straight', 'kurta', 'fo...</td>\n",
       "      <td>[10977, 21, 289, 222, 18034, 76, 2997, 1221, 6...</td>\n",
       "      <td>Folklore Women's Straight Kurta (FOKU001013_RE...</td>\n",
       "      <td>[40, 5, 11, 26, 11, 5, 7, 3, 2, 42, 5, 19, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1530678</td>\n",
       "      <td>gl_toy</td>\n",
       "      <td>['rock', 'party', 'elephant', 'piggy', 'coin',...</td>\n",
       "      <td>[1699, 128, 2210, 5256, 1189, 1552, 170, 312]</td>\n",
       "      <td>Rock The Party Elephant Piggy Coin Bank (Pink)...</td>\n",
       "      <td>[45, 5, 12, 26, 2, 30, 13, 3, 2, 27, 4, 7, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1324955</td>\n",
       "      <td>gl_apparel</td>\n",
       "      <td>['cherokee', 'unlimited', 'girl', 'shirt', '_N...</td>\n",
       "      <td>[1743, 1043, 82, 32, 1316, 1837, 3727, 4, 4, 1...</td>\n",
       "      <td>Cherokee by Unlimited Girls' T-Shirt (26362639...</td>\n",
       "      <td>[23, 13, 3, 7, 5, 26, 3, 3, 2, 24, 20, 2, 58, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>822454</td>\n",
       "      <td>gl_biss</td>\n",
       "      <td>['packingsupply', 'premium', 'tamper', 'proof'...</td>\n",
       "      <td>[1, 54, 3065, 228, 6192, 71, 4754, 19, 4, 4, 7...</td>\n",
       "      <td>Packingsupply Premium Tamper Proof Courier Bag...</td>\n",
       "      <td>[27, 4, 12, 26, 8, 9, 16, 10, 15, 18, 18, 11, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1604015</td>\n",
       "      <td>gl_shoes</td>\n",
       "      <td>['contablue', 'funky', 'loafer', '_NUM3_', 'bl...</td>\n",
       "      <td>[15592, 2110, 734, 4, 31, 7, 320, 118, 35, 235...</td>\n",
       "      <td>CONTABLUE Funky Loafers (8 UK, Black)[Material...</td>\n",
       "      <td>[23, 46, 49, 30, 34, 32, 38, 58, 43, 2, 40, 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID          GL                                               text  \\\n",
       "0  1585751  gl_apparel  ['folklore', 'woman', 'straight', 'kurta', 'fo...   \n",
       "1  1530678      gl_toy  ['rock', 'party', 'elephant', 'piggy', 'coin',...   \n",
       "2  1324955  gl_apparel  ['cherokee', 'unlimited', 'girl', 'shirt', '_N...   \n",
       "3   822454     gl_biss  ['packingsupply', 'premium', 'tamper', 'proof'...   \n",
       "4  1604015    gl_shoes  ['contablue', 'funky', 'loafer', '_NUM3_', 'bl...   \n",
       "\n",
       "                                        text_encoded  \\\n",
       "0  [10977, 21, 289, 222, 18034, 76, 2997, 1221, 6...   \n",
       "1      [1699, 128, 2210, 5256, 1189, 1552, 170, 312]   \n",
       "2  [1743, 1043, 82, 32, 1316, 1837, 3727, 4, 4, 1...   \n",
       "3  [1, 54, 3065, 228, 6192, 71, 4754, 19, 4, 4, 7...   \n",
       "4  [15592, 2110, 734, 4, 31, 7, 320, 118, 35, 235...   \n",
       "\n",
       "                                                char  \\\n",
       "0  Folklore Women's Straight Kurta (FOKU001013_RE...   \n",
       "1  Rock The Party Elephant Piggy Coin Bank (Pink)...   \n",
       "2  Cherokee by Unlimited Girls' T-Shirt (26362639...   \n",
       "3  Packingsupply Premium Tamper Proof Courier Bag...   \n",
       "4  CONTABLUE Funky Loafers (8 UK, Black)[Material...   \n",
       "\n",
       "                                        char_encoded  \n",
       "0  [40, 5, 11, 26, 11, 5, 7, 3, 2, 42, 5, 19, 3, ...  \n",
       "1  [45, 5, 12, 26, 2, 30, 13, 3, 2, 27, 4, 7, 6, ...  \n",
       "2  [23, 13, 3, 7, 5, 26, 3, 3, 2, 24, 20, 2, 58, ...  \n",
       "3  [27, 4, 12, 26, 8, 9, 16, 10, 15, 18, 18, 11, ...  \n",
       "4  [23, 46, 49, 30, 34, 32, 38, 58, 43, 2, 40, 15...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GL encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Categorical fit start at: 2019-06-13 04:25:47.154903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input to Neural Network: (61, 61), Output shape: (61, 69)\n",
      "Train on 37 samples, validate on 24 samples\n",
      "Epoch 1/200\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 0.2416 - val_loss: 0.2380\n",
      "Epoch 2/200\n",
      "37/37 [==============================] - 0s 56us/step - loss: 0.2368 - val_loss: 0.2329\n",
      "Epoch 3/200\n",
      "37/37 [==============================] - 0s 46us/step - loss: 0.2296 - val_loss: 0.2255\n",
      "Epoch 4/200\n",
      "37/37 [==============================] - 0s 44us/step - loss: 0.2193 - val_loss: 0.2155\n",
      "Epoch 5/200\n",
      "37/37 [==============================] - 0s 46us/step - loss: 0.2055 - val_loss: 0.2026\n",
      "Epoch 6/200\n",
      "37/37 [==============================] - 0s 45us/step - loss: 0.1881 - val_loss: 0.1867\n",
      "Epoch 7/200\n",
      "37/37 [==============================] - 0s 46us/step - loss: 0.1672 - val_loss: 0.1682\n",
      "Epoch 8/200\n",
      "37/37 [==============================] - 0s 47us/step - loss: 0.1437 - val_loss: 0.1476\n",
      "Epoch 9/200\n",
      "37/37 [==============================] - 0s 44us/step - loss: 0.1191 - val_loss: 0.1258\n",
      "Epoch 10/200\n",
      "37/37 [==============================] - 0s 45us/step - loss: 0.0950 - val_loss: 0.1041\n",
      "Epoch 11/200\n",
      "37/37 [==============================] - 0s 53us/step - loss: 0.0734 - val_loss: 0.0838\n",
      "Epoch 12/200\n",
      "37/37 [==============================] - 0s 44us/step - loss: 0.0555 - val_loss: 0.0661\n",
      "Epoch 13/200\n",
      "37/37 [==============================] - 0s 44us/step - loss: 0.0418 - val_loss: 0.0517\n",
      "Epoch 14/200\n",
      "37/37 [==============================] - 0s 45us/step - loss: 0.0322 - val_loss: 0.0407\n",
      "Epoch 15/200\n",
      "37/37 [==============================] - 0s 53us/step - loss: 0.0260 - val_loss: 0.0328\n",
      "Epoch 16/200\n",
      "37/37 [==============================] - 0s 44us/step - loss: 0.0222 - val_loss: 0.0273\n",
      "Epoch 17/200\n",
      "37/37 [==============================] - 0s 49us/step - loss: 0.0201 - val_loss: 0.0237\n",
      "Epoch 18/200\n",
      "37/37 [==============================] - 0s 45us/step - loss: 0.0191 - val_loss: 0.0214\n",
      "Epoch 19/200\n",
      "37/37 [==============================] - 0s 42us/step - loss: 0.0186 - val_loss: 0.0199\n",
      "Epoch 20/200\n",
      "37/37 [==============================] - 0s 47us/step - loss: 0.0184 - val_loss: 0.0189\n",
      "Epoch 21/200\n",
      "37/37 [==============================] - 0s 43us/step - loss: 0.0184 - val_loss: 0.0182\n",
      "Epoch 22/200\n",
      "37/37 [==============================] - 0s 52us/step - loss: 0.0183 - val_loss: 0.0178\n",
      "Epoch 23/200\n",
      "37/37 [==============================] - 0s 47us/step - loss: 0.0184 - val_loss: 0.0176\n",
      "Epoch 24/200\n",
      "37/37 [==============================] - 0s 51us/step - loss: 0.0184 - val_loss: 0.0174\n",
      "Epoch 25/200\n",
      "37/37 [==============================] - 0s 57us/step - loss: 0.0185 - val_loss: 0.0173\n",
      "Epoch 26/200\n",
      "37/37 [==============================] - 0s 44us/step - loss: 0.0185 - val_loss: 0.0173\n",
      "Epoch 27/200\n",
      "37/37 [==============================] - 0s 47us/step - loss: 0.0185 - val_loss: 0.0172\n",
      "Epoch 28/200\n",
      "37/37 [==============================] - 0s 49us/step - loss: 0.0185 - val_loss: 0.0173\n",
      "Epoch 29/200\n",
      "37/37 [==============================] - 0s 44us/step - loss: 0.0186 - val_loss: 0.0173\n",
      "Epoch 30/200\n",
      "37/37 [==============================] - 0s 43us/step - loss: 0.0186 - val_loss: 0.0173\n",
      "Epoch 31/200\n",
      "37/37 [==============================] - 0s 46us/step - loss: 0.0186 - val_loss: 0.0173\n",
      "Epoch 32/200\n",
      "37/37 [==============================] - 0s 58us/step - loss: 0.0186 - val_loss: 0.0173\n",
      "Train on 24 samples, validate on 37 samples\n",
      "Epoch 1/200\n",
      "24/24 [==============================] - 0s 68us/step - loss: 0.0173 - val_loss: 0.0186\n",
      "Epoch 2/200\n",
      "24/24 [==============================] - 0s 89us/step - loss: 0.0173 - val_loss: 0.0186\n",
      "Epoch 3/200\n",
      "24/24 [==============================] - 0s 66us/step - loss: 0.0172 - val_loss: 0.0187\n",
      "Epoch 4/200\n",
      "24/24 [==============================] - 0s 68us/step - loss: 0.0171 - val_loss: 0.0187\n",
      "Epoch 5/200\n",
      "24/24 [==============================] - 0s 69us/step - loss: 0.0171 - val_loss: 0.0188\n",
      "Epoch 6/200\n",
      "24/24 [==============================] - 0s 70us/step - loss: 0.0170 - val_loss: 0.0190\n",
      "Epoch 7/200\n",
      "24/24 [==============================] - 0s 70us/step - loss: 0.0170 - val_loss: 0.0191\n",
      "Neural Categorical fit done at: 2019-06-13 04:26:08.604625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<data_science_utils.preprocessing.NeuralCategoricalFeatureTransformer at 0x7f02a042fb38>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_science_utils.preprocessing import NeuralCategoricalFeatureTransformer\n",
    "\n",
    "ct_nn = NeuralCategoricalFeatureTransformer(cols=[\"GL\"],prefix=\"gl_encoded_\",\n",
    "                                            target_columns=[\"PRICE\"],verbose=1,n_components=16,n_iter=200,)\n",
    "\n",
    "ct_nn.fit(df_train)\n",
    "\n",
    "ct_nn.skip_fit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = ct_nn.transform(df_train)\n",
    "df_test = ct_nn.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_cols = get_specific_cols(df_train,prefix='gl_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "- GL Mean\n",
    "- GL Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T09:05:25.984142Z",
     "start_time": "2019-06-12T09:05:25.507678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1585751</td>\n",
       "      <td>772.612384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1530678</td>\n",
       "      <td>1290.724354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1324955</td>\n",
       "      <td>772.612384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>822454</td>\n",
       "      <td>2321.772340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1604015</td>\n",
       "      <td>1115.235239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID        PRICE\n",
       "0  1585751   772.612384\n",
       "1  1530678  1290.724354\n",
       "2  1324955   772.612384\n",
       "3   822454  2321.772340\n",
       "4  1604015  1115.235239"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gl_means = df_train.groupby([\"GL\"])[['PRICE']].mean().reset_index()\n",
    "\n",
    "df_results = df_test.merge(df_gl_means, on=[\"GL\"],how=\"left\")\n",
    "df_results = df_results[[\"ID\",\"PRICE\"]]\n",
    "df_results[\"PRICE\"] = df_results[\"PRICE\"].fillna(df_results[\"PRICE\"].mean())\n",
    "\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T09:05:50.559363Z",
     "start_time": "2019-06-12T09:05:49.247666Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results.to_csv(\"baseline.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## For Word Embedding CNN\n",
    "- Making a text column\n",
    "    - append GL\n",
    "    - replace num\n",
    "    - replace measurement\n",
    "    \n",
    "- Hyper Params here:\n",
    "    - Stopwords\n",
    "    - word_length_filter\n",
    "    - lemmatize or not\n",
    "    - vocab_size in build_dict\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline 2: Keras Imdb Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159686/1159686 [==============================] - 341s 294us/step - loss: 4033933786.3938 - mean_absolute_error: 894.5309 - val_loss: 16347761.4813 - val_mean_absolute_error: 716.5794\n",
      "Epoch 2/5\n",
      "1159686/1159686 [==============================] - 340s 293us/step - loss: 4032204864.3942 - mean_absolute_error: 822.3543 - val_loss: 15832785.5048 - val_mean_absolute_error: 704.4332\n",
      "Epoch 3/5\n",
      "1159686/1159686 [==============================] - 339s 292us/step - loss: 4031421547.0617 - mean_absolute_error: 794.2296 - val_loss: 15649164.7560 - val_mean_absolute_error: 698.5270\n",
      "Epoch 4/5\n",
      "1159686/1159686 [==============================] - 341s 294us/step - loss: 4030308633.8232 - mean_absolute_error: 781.7042 - val_loss: 15564212.7209 - val_mean_absolute_error: 678.7286\n",
      "Epoch 5/5\n",
      "1159686/1159686 [==============================] - 342s 295us/step - loss: 4028990347.3919 - mean_absolute_error: 773.1664 - val_loss: 15462277.1944 - val_mean_absolute_error: 689.7498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbc8e2027b8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "# set parameters:\n",
    "max_features = 50000\n",
    "maxlen = 100\n",
    "batch_size = 256\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 5\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "X,y = df_train['text_encoded'].values,df_train['PRICE'].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['mae','mse'])\n",
    "model.count_params()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2604751"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 1159686 samples, validate on 289922 samples\n",
      "Epoch 1/5\n",
      "1159686/1159686 [==============================] - 340s 293us/step - loss: 4034018597.3883 - mean_absolute_error: 897.5453 - mean_squared_error: 4034018597.3883 - val_loss: 16583135.4411 - val_mean_absolute_error: 736.9392 - val_mean_squared_error: 16583135.4411\n",
      "Epoch 2/5\n",
      "1159686/1159686 [==============================] - 339s 292us/step - loss: 4032405869.1175 - mean_absolute_error: 823.1647 - mean_squared_error: 4032405869.1175 - val_loss: 15923565.7908 - val_mean_absolute_error: 710.7797 - val_mean_squared_error: 15923565.7908\n",
      "Epoch 3/5\n",
      "1159686/1159686 [==============================] - 344s 296us/step - loss: 4031477803.3825 - mean_absolute_error: 802.0114 - mean_squared_error: 4031477803.3825 - val_loss: 15712283.7967 - val_mean_absolute_error: 673.6900 - val_mean_squared_error: 15712283.7967\n",
      "Epoch 4/5\n",
      "1159686/1159686 [==============================] - 377s 325us/step - loss: 4030602274.8077 - mean_absolute_error: 787.0241 - mean_squared_error: 4030602274.8077 - val_loss: 15536185.3586 - val_mean_absolute_error: 684.5243 - val_mean_squared_error: 15536185.3586\n",
      "Epoch 5/5\n",
      "1159686/1159686 [==============================] - 355s 306us/step - loss: 4028773892.0582 - mean_absolute_error: 774.2108 - mean_squared_error: 4028773892.0582 - val_loss: 15547249.8296 - val_mean_absolute_error: 661.7055 - val_mean_squared_error: 15547249.8296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efed85b0cc0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "# set parameters:\n",
    "max_features = 50000\n",
    "maxlen = 100\n",
    "batch_size = 256\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 5\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "X,X_gl,y = df_train['text_encoded'].values,df_train[gl_cols],df_train['PRICE'].values\n",
    "x_train, x_test,x_gl_train,x_gl_test, y_train, y_test = train_test_split(X,X_gl, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "main_input = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen)(main_input)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "x = Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1)(x)\n",
    "# we use max pooling:\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "auxiliary_input =  Input(shape=(len(gl_cols),), dtype='float32', name='aux_input')\n",
    "x = concatenate([x,auxiliary_input])\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = Dense(hidden_dims)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x =Activation('relu')(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "main_output = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output])\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['mae','mse'])\n",
    "model.count_params()\n",
    "\n",
    "model.fit([x_train,x_gl_train], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=([x_test,x_gl_test], y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "# set parameters:\n",
    "max_features = 50000\n",
    "maxlen = 100\n",
    "batch_size = 256\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 5\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "X,X_gl,y = df_train['text_encoded'].values,df_train[gl_cols],df_train['PRICE'].values\n",
    "x_train, x_test,x_gl_train,x_gl_test, y_train, y_test = train_test_split(X,X_gl, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "main_input = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen)(main_input)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "\n",
    "x = Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1)(x)\n",
    "\n",
    "xp = MaxPooling1D()(x)\n",
    "x = transition_layer(x, n_kernels=64,dropout=0)\n",
    "x = MaxPooling1D()(x)\n",
    "x = grouped_layer(x, group_configs=[[dict(n_kernels=32, kernel_size=3, dropout=0.1,dilation_rate=1, padding='same')],\n",
    "                                    [dict(n_kernels=32, kernel_size=3, dropout=0.1,dilation_rate=3, padding='same')]],\n",
    "                 out_channels = 32)\n",
    "\n",
    "x = concatenate([x,xp])\n",
    "# we use max pooling:\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "auxiliary_input =  Input(shape=(len(gl_cols),), dtype='float32', name='aux_input')\n",
    "x = concatenate([x,auxiliary_input])\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = Dense(hidden_dims)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x =Activation('relu')(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "main_output = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output])\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['mae','mse'])\n",
    "model.count_params()\n",
    "\n",
    "model.fit([x_train,x_gl_train], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=([x_test,x_gl_test], y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "X_test = df_test['text_encoded'].values\n",
    "X_test_gl = df_test[gl_cols]\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "y_preds = model.predict([X_test,X_test_gl])\n",
    "\n",
    "df_results = df_test[['ID']]\n",
    "df_results['PRICE'] = y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1585751</td>\n",
       "      <td>289.457550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1530678</td>\n",
       "      <td>748.071350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1324955</td>\n",
       "      <td>289.457550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>822454</td>\n",
       "      <td>1509.970459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1604015</td>\n",
       "      <td>625.523804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID        PRICE\n",
       "0  1585751   289.457550\n",
       "1  1530678   748.071350\n",
       "2  1324955   289.457550\n",
       "3   822454  1509.970459\n",
       "4  1604015   625.523804"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.head()\n",
    "df_results.to_csv(\"baseline-3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 50000\n",
    "maxlen = 100\n",
    "batch_size = 4096\n",
    "embedding_dims = 50\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "X,X_gl,y = df_train['text_encoded'].values,df_train[gl_cols],df_train['PRICE'].values\n",
    "x_train, x_test,x_gl_train,x_gl_test, y_train, y_test = train_test_split(X,X_gl, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "main_input = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(output_dim=embedding_dims, input_dim=max_features, input_length=maxlen)(main_input)\n",
    "x = conv_layer(x,n_kernels=64,kernel_size=5,padding='valid')\n",
    "x = transition_layer(x, n_kernels=32,dropout=0)\n",
    "x = MaxPooling1D()(x)\n",
    "x = conv_layer(x,n_kernels=256,kernel_size=3,padding='valid')\n",
    "x = MaxPooling1D()(x)\n",
    "K.int_shape(x)\n",
    "x = transition_layer(x, n_kernels=64,dropout=0)\n",
    "\n",
    "x = pre_dense_layer(x)\n",
    "auxiliary_input =  Input(shape=(len(gl_cols),), dtype='float32', name='aux_input')\n",
    "x = concatenate([x,auxiliary_input])\n",
    "\n",
    "x = Dense(64)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x =Activation('relu')(x)\n",
    "\n",
    "main_output = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output])\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model.hdf5\", monitor='mse', verbose=0, save_best_only=True, mode='max')\n",
    "lr_manager = OneCycleLR(samples=x_train.shape[0], epochs=2, batch_size=batch_size,\n",
    "                        steps=int(np.floor(x_train.shape[0]/batch_size)), max_lr=0.001,\n",
    "                        end_percentage=0.1, scale_percentage=None,\n",
    "                        maximum_momentum=None, minimum_momentum=None)\n",
    "\n",
    "callbacks_list = [lr_manager,checkpoint]\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['mae','mse'])\n",
    "print(\"Params = \",model.count_params())\n",
    "\n",
    "model.fit([x_train,x_gl_train], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,callbacks=callbacks_list,\n",
    "          validation_data=([x_test,x_gl_test], y_test))\n",
    "\n",
    "\n",
    "model.load_weights(\"model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 23, 256)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params =  2655841\n",
      "Train on 1159686 samples, validate on 289922 samples\n",
      "Epoch 1/5\n",
      "1159686/1159686 [==============================] - 452s 390us/step - loss: 4036263306.2327 - mean_absolute_error: 1006.4514 - mean_squared_error: 4036263306.2327 - val_loss: 17005608.9154 - val_mean_absolute_error: 1143.8958 - val_mean_squared_error: 17005608.9154\n",
      " - lr: 0.00090 \n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with mse available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 360448/1159686 [========>.....................] - ETA: 4:40 - loss: 16153633.2188 - mean_absolute_error: 685.1413 - mean_squared_error: 16153633.2188"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-e7406521a4f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m           validation_data=([x_test,x_gl_test], y_test))\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_features = 50000\n",
    "maxlen = 100\n",
    "batch_size = 4096\n",
    "embedding_dims = 50\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "X,X_gl,y = df_train['text_encoded'].values,df_train[gl_cols],df_train['PRICE'].values\n",
    "x_train, x_test,x_gl_train,x_gl_test, y_train, y_test = train_test_split(X,X_gl, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "main_input = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(output_dim=embedding_dims, input_dim=max_features, input_length=maxlen)(main_input)\n",
    "\n",
    "\n",
    "\n",
    "x = conv_layer(x,n_kernels=64,kernel_size=5,padding='valid')\n",
    "x = transition_layer(x, n_kernels=32,dropout=0)\n",
    "x = MaxPooling1D()(x)\n",
    "xp = x\n",
    "\n",
    "x = grouped_layer(x, group_configs=[[dict(n_kernels=64, kernel_size=3, dropout=0.1,dilation_rate=1, padding='same')],\n",
    "                                    [dict(n_kernels=64, kernel_size=3, dropout=0.1,dilation_rate=1, padding='same'),dict(n_kernels=32, kernel_size=3, dropout=0.1,dilation_rate=1, padding='same')],\n",
    "                                    [dict(n_kernels=64, kernel_size=3, dropout=0.1,dilation_rate=2, padding='same')]],\n",
    "                 out_channels = 64)\n",
    "\n",
    "x = concatenate([x,xp])\n",
    "\n",
    "x = conv_layer(x,n_kernels=256,kernel_size=3,padding='valid')\n",
    "x = MaxPooling1D()(x)\n",
    "K.int_shape(x)\n",
    "x = transition_layer(x, n_kernels=64,dropout=0)\n",
    "\n",
    "x = pre_dense_layer(x)\n",
    "auxiliary_input =  Input(shape=(len(gl_cols),), dtype='float32', name='aux_input')\n",
    "x = concatenate([x,auxiliary_input])\n",
    "\n",
    "x = Dense(64)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x =Activation('relu')(x)\n",
    "\n",
    "main_output = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output])\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model.hdf5\", monitor='mse', verbose=0, save_best_only=True, mode='max')\n",
    "lr_manager = OneCycleLR(samples=x_train.shape[0], epochs=2, batch_size=batch_size,\n",
    "                        steps=int(np.floor(x_train.shape[0]/batch_size)), max_lr=0.001,\n",
    "                        end_percentage=0.1, scale_percentage=None,\n",
    "                        maximum_momentum=None, minimum_momentum=None)\n",
    "\n",
    "callbacks_list = [lr_manager,checkpoint]\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['mae','mse'])\n",
    "print(\"Params = \",model.count_params())\n",
    "\n",
    "model.fit([x_train,x_gl_train], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,callbacks=callbacks_list,\n",
    "          validation_data=([x_test,x_gl_test], y_test))\n",
    "\n",
    "\n",
    "model.load_weights(\"model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 128\n",
    "maxlen = 500\n",
    "batch_size = 1024\n",
    "embedding_dims = 20\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "X,X_gl,y = df_train['char_encoded'].values,df_train[gl_cols],df_train['PRICE'].values\n",
    "x_train, x_test,x_gl_train,x_gl_test, y_train, y_test = train_test_split(X,X_gl, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "main_input = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(output_dim=embedding_dims, input_dim=max_features, input_length=maxlen)(main_input)\n",
    "x = conv_layer(x,n_kernels=32,kernel_size=25,padding='valid')\n",
    "x = MaxPooling1D()(x)\n",
    "x = conv_layer(x,n_kernels=64,kernel_size=15,padding='valid')\n",
    "x = transition_layer(x, n_kernels=32,dropout=0)\n",
    "x = MaxPooling1D()(x)\n",
    "x = conv_layer(x,n_kernels=128,kernel_size=15,padding='valid')\n",
    "x = transition_layer(x, n_kernels=64,dropout=0)\n",
    "x = MaxPooling1D()(x)\n",
    "\n",
    "K.int_shape(x)\n",
    "x = pre_dense_layer(x)\n",
    "K.int_shape(x)\n",
    "auxiliary_input =  Input(shape=(len(gl_cols),), dtype='float32', name='aux_input')\n",
    "x = concatenate([x,auxiliary_input])\n",
    "\n",
    "x = Dense(64)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x =Activation('relu')(x)\n",
    "main_output = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output])\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model.hdf5\", monitor='mse', verbose=0, save_best_only=True, mode='max')\n",
    "lr_manager = OneCycleLR(samples=x_train.shape[0], epochs=2, batch_size=batch_size,\n",
    "                        steps=int(np.floor(x_train.shape[0]/batch_size)), max_lr=0.01,\n",
    "                        end_percentage=0.1, scale_percentage=None,\n",
    "                        maximum_momentum=None, minimum_momentum=None)\n",
    "\n",
    "callbacks_list = [lr_manager,checkpoint]\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['mae','mse'])\n",
    "print(\"Params = \",model.count_params())\n",
    "\n",
    "model.fit([x_train,x_gl_train], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,callbacks=callbacks_list,\n",
    "          validation_data=([x_test,x_gl_test], y_test))\n",
    "\n",
    "\n",
    "model.load_weights(\"model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing\n",
    "- gl min max clipping\n",
    "- max(0,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
