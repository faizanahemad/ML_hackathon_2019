{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T10:56:06.982560Z",
     "start_time": "2019-06-12T10:56:06.943268Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np_utils\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, DepthwiseConv2D, Conv2D, SeparableConv2D, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers import Input, concatenate\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Nadam, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.regularizers import l2\n",
    "%config InlineBackend.figure_format='retina'\n",
    "from keras_contrib.callbacks import CyclicLR\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from data_science_utils.vision.keras import *\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import missingno as msno\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "from data_science_utils import dataframe as df_utils\n",
    "from data_science_utils import models as model_utils\n",
    "from data_science_utils import plots as plot_utils\n",
    "from data_science_utils.dataframe import column as column_utils\n",
    "from data_science_utils import misc as misc\n",
    "from data_science_utils import preprocessing as pp_utils\n",
    "from data_science_utils import nlp as nlp_utils\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from data_science_utils.dataframe import get_specific_cols\n",
    "\n",
    "import more_itertools\n",
    "from more_itertools import flatten\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lib' from '/home/ec2-user/SageMaker/ML_hackathon_2019/lib.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "from importlib import reload\n",
    "import lib\n",
    "reload(lib)\n",
    "from lib import *\n",
    "\n",
    "from oclr import OneCycleLR, LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T09:42:02.848654Z",
     "start_time": "2019-06-12T09:41:52.143642Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"price_prediction/train.csv\")\n",
    "df_test = pd.read_csv(\"price_prediction/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6351347bc380400a893bd8d3991704ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1449608), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8bde5cbe0546c8b08dc3f1a7f6c7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1449608), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6b90ce0a584d9a82b1e82c9060db20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1449608), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0465d2695fa4d258e0806239130a8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=362403), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cbd2e5e8664194bfc5e1c49715da4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=362403), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecf6fb537fd4c769b8a13e6eb1ad965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=362403), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_train['text'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_train['text']))\n",
    "df_train['text_encoded'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_train['text_encoded']))\n",
    "df_train['char_encoded'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_train['char_encoded']))\n",
    "\n",
    "df_test['text'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_test['text']))\n",
    "df_test['text_encoded'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_test['text_encoded']))\n",
    "df_test['char_encoded'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_test['char_encoded']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T10:40:19.816205Z",
     "start_time": "2019-06-12T10:39:01.196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>GL</th>\n",
       "      <th>text</th>\n",
       "      <th>text_encoded</th>\n",
       "      <th>char</th>\n",
       "      <th>char_encoded</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1489960</td>\n",
       "      <td>gl_jewelry</td>\n",
       "      <td>['livsmart', 'resin', 'jewellery', 'display', ...</td>\n",
       "      <td>[28683, 1558, 273, 310, 196, 8, 4, 31, 453, 33...</td>\n",
       "      <td>Livsmart Resin Jewellery Display Stand, 17x11c...</td>\n",
       "      <td>[38, 8, 28, 10, 19, 4, 7, 6, 2, 45, 3, 10, 8, ...</td>\n",
       "      <td>507.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>633493</td>\n",
       "      <td>gl_digital_ebook_purchase</td>\n",
       "      <td>['quantum', 'creation', 'supernatural', 'lurk'...</td>\n",
       "      <td>[7567, 879, 12991, 1, 3900, 207, 267]</td>\n",
       "      <td>Quantum Creation: Does the Supernatural Lurk i...</td>\n",
       "      <td>[78, 15, 4, 9, 6, 15, 19, 2, 23, 7, 3, 4, 6, 8...</td>\n",
       "      <td>479.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1474591</td>\n",
       "      <td>gl_apparel</td>\n",
       "      <td>['izod', 'men', 'casual', 'shirt', '_NUM30_', ...</td>\n",
       "      <td>[5244, 24, 35, 32, 127, 26671, 64, 5368, 9, 90...</td>\n",
       "      <td>IZOD Men's Casual Shirt (8907163477392_ZKSH019...</td>\n",
       "      <td>[41, 81, 46, 39, 2, 29, 3, 9, 63, 10, 2, 23, 4...</td>\n",
       "      <td>829.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>830218</td>\n",
       "      <td>gl_apparel</td>\n",
       "      <td>['dishaa', 'woman', 'rayon', 'line', 'kurta', ...</td>\n",
       "      <td>[45999, 21, 572, 192, 222, 31, 133, 2162, 369,...</td>\n",
       "      <td>Dishaa Women's Rayon A-Line Kurta (Black, X-La...</td>\n",
       "      <td>[39, 8, 10, 13, 4, 4, 2, 42, 5, 19, 3, 9, 63, ...</td>\n",
       "      <td>648.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201144</td>\n",
       "      <td>gl_digital_ebook_purchase</td>\n",
       "      <td>['return', 'raven', 'ulfrik', 'ormsson', 'saga...</td>\n",
       "      <td>[854, 13891, 1, 1, 7870, 597, 2, 267]</td>\n",
       "      <td>Return of the Ravens (Ulfrik Ormsson's Saga Bo...</td>\n",
       "      <td>[45, 3, 6, 15, 7, 9, 2, 5, 21, 2, 6, 13, 3, 2,...</td>\n",
       "      <td>332.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                         GL  \\\n",
       "0  1489960                 gl_jewelry   \n",
       "1   633493  gl_digital_ebook_purchase   \n",
       "2  1474591                 gl_apparel   \n",
       "3   830218                 gl_apparel   \n",
       "4   201144  gl_digital_ebook_purchase   \n",
       "\n",
       "                                                text  \\\n",
       "0  ['livsmart', 'resin', 'jewellery', 'display', ...   \n",
       "1  ['quantum', 'creation', 'supernatural', 'lurk'...   \n",
       "2  ['izod', 'men', 'casual', 'shirt', '_NUM30_', ...   \n",
       "3  ['dishaa', 'woman', 'rayon', 'line', 'kurta', ...   \n",
       "4  ['return', 'raven', 'ulfrik', 'ormsson', 'saga...   \n",
       "\n",
       "                                        text_encoded  \\\n",
       "0  [28683, 1558, 273, 310, 196, 8, 4, 31, 453, 33...   \n",
       "1              [7567, 879, 12991, 1, 3900, 207, 267]   \n",
       "2  [5244, 24, 35, 32, 127, 26671, 64, 5368, 9, 90...   \n",
       "3  [45999, 21, 572, 192, 222, 31, 133, 2162, 369,...   \n",
       "4              [854, 13891, 1, 1, 7870, 597, 2, 267]   \n",
       "\n",
       "                                                char  \\\n",
       "0  Livsmart Resin Jewellery Display Stand, 17x11c...   \n",
       "1  Quantum Creation: Does the Supernatural Lurk i...   \n",
       "2  IZOD Men's Casual Shirt (8907163477392_ZKSH019...   \n",
       "3  Dishaa Women's Rayon A-Line Kurta (Black, X-La...   \n",
       "4  Return of the Ravens (Ulfrik Ormsson's Saga Bo...   \n",
       "\n",
       "                                        char_encoded   PRICE  \n",
       "0  [38, 8, 28, 10, 19, 4, 7, 6, 2, 45, 3, 10, 8, ...  507.62  \n",
       "1  [78, 15, 4, 9, 6, 15, 19, 2, 23, 7, 3, 4, 6, 8...  479.90  \n",
       "2  [41, 81, 46, 39, 2, 29, 3, 9, 63, 10, 2, 23, 4...  829.28  \n",
       "3  [39, 8, 10, 13, 4, 4, 2, 42, 5, 19, 3, 9, 63, ...  648.31  \n",
       "4  [45, 3, 6, 15, 7, 9, 2, 5, 21, 2, 6, 13, 3, 2,...  332.00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>GL</th>\n",
       "      <th>text</th>\n",
       "      <th>text_encoded</th>\n",
       "      <th>char</th>\n",
       "      <th>char_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1585751</td>\n",
       "      <td>gl_apparel</td>\n",
       "      <td>['folklore', 'woman', 'straight', 'kurta', 'fo...</td>\n",
       "      <td>[10977, 21, 289, 222, 18034, 76, 2997, 1221, 6...</td>\n",
       "      <td>Folklore Women's Straight Kurta (FOKU001013_RE...</td>\n",
       "      <td>[40, 5, 11, 26, 11, 5, 7, 3, 2, 42, 5, 19, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1530678</td>\n",
       "      <td>gl_toy</td>\n",
       "      <td>['rock', 'party', 'elephant', 'piggy', 'coin',...</td>\n",
       "      <td>[1699, 128, 2210, 5256, 1189, 1552, 170, 312]</td>\n",
       "      <td>Rock The Party Elephant Piggy Coin Bank (Pink)...</td>\n",
       "      <td>[45, 5, 12, 26, 2, 30, 13, 3, 2, 27, 4, 7, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1324955</td>\n",
       "      <td>gl_apparel</td>\n",
       "      <td>['cherokee', 'unlimited', 'girl', 'shirt', '_N...</td>\n",
       "      <td>[1743, 1043, 82, 32, 1316, 1837, 3727, 4, 4, 1...</td>\n",
       "      <td>Cherokee by Unlimited Girls' T-Shirt (26362639...</td>\n",
       "      <td>[23, 13, 3, 7, 5, 26, 3, 3, 2, 24, 20, 2, 58, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>822454</td>\n",
       "      <td>gl_biss</td>\n",
       "      <td>['packingsupply', 'premium', 'tamper', 'proof'...</td>\n",
       "      <td>[1, 54, 3065, 228, 6192, 71, 4754, 19, 4, 4, 7...</td>\n",
       "      <td>Packingsupply Premium Tamper Proof Courier Bag...</td>\n",
       "      <td>[27, 4, 12, 26, 8, 9, 16, 10, 15, 18, 18, 11, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1604015</td>\n",
       "      <td>gl_shoes</td>\n",
       "      <td>['contablue', 'funky', 'loafer', '_NUM3_', 'bl...</td>\n",
       "      <td>[15592, 2110, 734, 4, 31, 7, 320, 118, 35, 235...</td>\n",
       "      <td>CONTABLUE Funky Loafers (8 UK, Black)[Material...</td>\n",
       "      <td>[23, 46, 49, 30, 34, 32, 38, 58, 43, 2, 40, 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID          GL                                               text  \\\n",
       "0  1585751  gl_apparel  ['folklore', 'woman', 'straight', 'kurta', 'fo...   \n",
       "1  1530678      gl_toy  ['rock', 'party', 'elephant', 'piggy', 'coin',...   \n",
       "2  1324955  gl_apparel  ['cherokee', 'unlimited', 'girl', 'shirt', '_N...   \n",
       "3   822454     gl_biss  ['packingsupply', 'premium', 'tamper', 'proof'...   \n",
       "4  1604015    gl_shoes  ['contablue', 'funky', 'loafer', '_NUM3_', 'bl...   \n",
       "\n",
       "                                        text_encoded  \\\n",
       "0  [10977, 21, 289, 222, 18034, 76, 2997, 1221, 6...   \n",
       "1      [1699, 128, 2210, 5256, 1189, 1552, 170, 312]   \n",
       "2  [1743, 1043, 82, 32, 1316, 1837, 3727, 4, 4, 1...   \n",
       "3  [1, 54, 3065, 228, 6192, 71, 4754, 19, 4, 4, 7...   \n",
       "4  [15592, 2110, 734, 4, 31, 7, 320, 118, 35, 235...   \n",
       "\n",
       "                                                char  \\\n",
       "0  Folklore Women's Straight Kurta (FOKU001013_RE...   \n",
       "1  Rock The Party Elephant Piggy Coin Bank (Pink)...   \n",
       "2  Cherokee by Unlimited Girls' T-Shirt (26362639...   \n",
       "3  Packingsupply Premium Tamper Proof Courier Bag...   \n",
       "4  CONTABLUE Funky Loafers (8 UK, Black)[Material...   \n",
       "\n",
       "                                        char_encoded  \n",
       "0  [40, 5, 11, 26, 11, 5, 7, 3, 2, 42, 5, 19, 3, ...  \n",
       "1  [45, 5, 12, 26, 2, 30, 13, 3, 2, 27, 4, 7, 6, ...  \n",
       "2  [23, 13, 3, 7, 5, 26, 3, 3, 2, 24, 20, 2, 58, ...  \n",
       "3  [27, 4, 12, 26, 8, 9, 16, 10, 15, 18, 18, 11, ...  \n",
       "4  [23, 46, 49, 30, 34, 32, 38, 58, 43, 2, 40, 15...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GL encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Categorical fit start at: 2019-06-13 08:23:04.386101\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input to Neural Network: (61, 61), Output shape: (61, 69)\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Neural Categorical fit done at: 2019-06-13 08:23:40.093439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<data_science_utils.preprocessing.NeuralCategoricalFeatureTransformer at 0x7f9926460898>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_science_utils.preprocessing import NeuralCategoricalFeatureTransformer\n",
    "\n",
    "ct_nn = NeuralCategoricalFeatureTransformer(cols=[\"GL\"],prefix=\"gl_encoded_\",\n",
    "                                            target_columns=[\"PRICE\"],verbose=0,n_components=16,n_iter=200,)\n",
    "\n",
    "ct_nn.fit(df_train)\n",
    "\n",
    "ct_nn.skip_fit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = ct_nn.transform(df_train)\n",
    "df_test = ct_nn.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_cols = get_specific_cols(df_train,prefix='gl_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "- GL Mean\n",
    "- GL Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T09:05:25.984142Z",
     "start_time": "2019-06-12T09:05:25.507678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1585751</td>\n",
       "      <td>772.612384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1530678</td>\n",
       "      <td>1290.724354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1324955</td>\n",
       "      <td>772.612384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>822454</td>\n",
       "      <td>2321.772340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1604015</td>\n",
       "      <td>1115.235239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID        PRICE\n",
       "0  1585751   772.612384\n",
       "1  1530678  1290.724354\n",
       "2  1324955   772.612384\n",
       "3   822454  2321.772340\n",
       "4  1604015  1115.235239"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gl_means = df_train.groupby([\"GL\"])[['PRICE']].mean().reset_index()\n",
    "\n",
    "df_results = df_test.merge(df_gl_means, on=[\"GL\"],how=\"left\")\n",
    "df_results = df_results[[\"ID\",\"PRICE\"]]\n",
    "df_results[\"PRICE\"] = df_results[\"PRICE\"].fillna(df_results[\"PRICE\"].mean())\n",
    "\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T09:05:50.559363Z",
     "start_time": "2019-06-12T09:05:49.247666Z"
    }
   },
   "outputs": [],
   "source": [
    "df_results.to_csv(\"baseline.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline 2: Keras Model\n",
    "\n",
    "\n",
    "For Word Embedding CNN\n",
    "\n",
    "- Making a text column\n",
    "    - append GL\n",
    "    - replace num\n",
    "    - replace measurement\n",
    "    \n",
    "- Hyper Params here:\n",
    "    - Stopwords\n",
    "    - word_length_filter\n",
    "    - lemmatize or not\n",
    "    - vocab_size in build_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159686/1159686 [==============================] - 341s 294us/step - loss: 4033933786.3938 - mean_absolute_error: 894.5309 - val_loss: 16347761.4813 - val_mean_absolute_error: 716.5794\n",
      "Epoch 2/5\n",
      "1159686/1159686 [==============================] - 340s 293us/step - loss: 4032204864.3942 - mean_absolute_error: 822.3543 - val_loss: 15832785.5048 - val_mean_absolute_error: 704.4332\n",
      "Epoch 3/5\n",
      "1159686/1159686 [==============================] - 339s 292us/step - loss: 4031421547.0617 - mean_absolute_error: 794.2296 - val_loss: 15649164.7560 - val_mean_absolute_error: 698.5270\n",
      "Epoch 4/5\n",
      "1159686/1159686 [==============================] - 341s 294us/step - loss: 4030308633.8232 - mean_absolute_error: 781.7042 - val_loss: 15564212.7209 - val_mean_absolute_error: 678.7286\n",
      "Epoch 5/5\n",
      "1159686/1159686 [==============================] - 342s 295us/step - loss: 4028990347.3919 - mean_absolute_error: 773.1664 - val_loss: 15462277.1944 - val_mean_absolute_error: 689.7498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbc8e2027b8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "# set parameters:\n",
    "max_features = 50000\n",
    "maxlen = 100\n",
    "batch_size = 256\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 5\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "X,y = df_train['text_encoded'].values,df_train['PRICE'].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['mae','mse'])\n",
    "model.count_params()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "# set parameters:\n",
    "max_features = 50000\n",
    "maxlen = 100\n",
    "batch_size = 256\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 5\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "X,X_gl,y = df_train['text_encoded'].values,df_train[gl_cols],df_train['PRICE'].values\n",
    "x_train, x_test,x_gl_train,x_gl_test, y_train, y_test = train_test_split(X,X_gl, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "main_input = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen)(main_input)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "x = Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1)(x)\n",
    "# we use max pooling:\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "auxiliary_input =  Input(shape=(len(gl_cols),), dtype='float32', name='aux_input')\n",
    "x = concatenate([x,auxiliary_input])\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = Dense(hidden_dims)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x =Activation('relu')(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "main_output = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output])\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['mae','mse'])\n",
    "model.count_params()\n",
    "\n",
    "model.fit([x_train,x_gl_train], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=([x_test,x_gl_test], y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Grouped Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2749299"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1159686 samples, validate on 289922 samples\n",
      "Epoch 1/5\n",
      "1159686/1159686 [==============================] - 185s 159us/step - loss: 4033730646.5426 - mean_absolute_error: 804.3981 - mean_squared_error: 4033730646.5426 - val_loss: 16842111.5503 - val_mean_absolute_error: 686.5982 - val_mean_squared_error: 16842111.5503\n",
      "Epoch 3/5\n",
      "1159686/1159686 [==============================] - 185s 159us/step - loss: 4032106983.2758 - mean_absolute_error: 810.7816 - mean_squared_error: 4032106983.2758 - val_loss: 16408113.0508 - val_mean_absolute_error: 685.7801 - val_mean_squared_error: 16408113.0508\n",
      "Epoch 4/5\n",
      "1159686/1159686 [==============================] - 185s 159us/step - loss: 4030989630.1320 - mean_absolute_error: 840.2338 - mean_squared_error: 4030989630.1320 - val_loss: 16202011.9769 - val_mean_absolute_error: 687.1333 - val_mean_squared_error: 16202011.9769\n",
      "Epoch 5/5\n",
      "1159686/1159686 [==============================] - 185s 160us/step - loss: 4030299779.8140 - mean_absolute_error: 852.5429 - mean_squared_error: 4030299779.8140 - val_loss: 16192004.1726 - val_mean_absolute_error: 688.8354 - val_mean_squared_error: 16192004.1726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f93f4189fd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "gc.collect()\n",
    "# set parameters:\n",
    "max_features = 50000\n",
    "maxlen = 100\n",
    "batch_size = 2048\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 5\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "X,X_gl,y = df_train['text_encoded'].values,df_train[gl_cols],df_train['PRICE'].values\n",
    "x_train, x_test,x_gl_train,x_gl_test, y_train, y_test = train_test_split(X,X_gl, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "main_input = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen)(main_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "\n",
    "x = Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1)(x)\n",
    "xp = x\n",
    "x = transition_layer(x, n_kernels=64,dropout=0)\n",
    "x = grouped_layer(x, group_configs=[[dict(n_kernels=64, kernel_size=3, dropout=0.1,dilation_rate=1, padding='same')],\n",
    "                                    [dict(n_kernels=64, kernel_size=3, dropout=0.1,dilation_rate=2, padding='same')]],\n",
    "                 out_channels = 64)\n",
    "\n",
    "x = concatenate([x,xp])\n",
    "# we use max pooling:\n",
    "x2 = GlobalAveragePooling1D()(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = concatenate([x,x2])\n",
    "\n",
    "auxiliary_input =  Input(shape=(len(gl_cols),), dtype='float32', name='aux_input')\n",
    "x = concatenate([x,auxiliary_input])\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = Dense(hidden_dims)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "main_output = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output])\n",
    "\n",
    "optimizer = Adam(lr=0.0001,)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mae','mse'])\n",
    "model.count_params()\n",
    "\n",
    "model.fit([x_train,x_gl_train], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=([x_test,x_gl_test], y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Train on 1159686 samples, validate on 289922 samples\n",
    "Epoch 1/5\n",
    "1159686/1159686 [==============================] - 180s 156us/step - loss: 4036435575.9622 - mean_absolute_error: 969.9626 - mean_squared_error: 4036435575.9622 - val_loss: 19007902.2033 - val_mean_absolute_error: 849.3713 - val_mean_squared_error: 19007902.2033\n",
    "Epoch 2/5\n",
    "1159686/1159686 [==============================] - 177s 152us/step - loss: 4034018067.3774 - mean_absolute_error: 802.3672 - mean_squared_error: 4034018067.3774 - val_loss: 17264962.1246 - val_mean_absolute_error: 689.0970 - val_mean_squared_error: 17264962.1246\n",
    "Epoch 3/5\n",
    "1159686/1159686 [==============================] - 176s 152us/step - loss: 4032383173.0434 - mean_absolute_error: 809.1992 - mean_squared_error: 4032383173.0434 - val_loss: 16797398.9764 - val_mean_absolute_error: 680.6902 - val_mean_squared_error: 16797398.9764\n",
    "Epoch 4/5\n",
    "1159686/1159686 [==============================] - 176s 152us/step - loss: 4028829368.1799 - mean_absolute_error: 830.8879 - mean_squared_error: 4028829368.1799 - val_loss: 16374906.9715 - val_mean_absolute_error: 685.6106 - val_mean_squared_error: 16374906.9715\n",
    "Epoch 5/5\n",
    "1159686/1159686 [==============================] - 176s 152us/step - loss: 4025341286.7346 - mean_absolute_error: 850.8240 - mean_squared_error: 4025341286.7346 - val_loss: 16511555.2654 - val_mean_absolute_error: 695.2230 - val_mean_squared_error: 16511555.2654\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2846875"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1159686 samples, validate on 289922 samples\n",
      "Epoch 1/5\n",
      "1159686/1159686 [==============================] - 155s 134us/step - loss: 4036715472.7262 - mean_absolute_error: 983.9760 - mean_squared_error: 4036715472.7262 - val_loss: 19128076.6866 - val_mean_absolute_error: 842.7234 - val_mean_squared_error: 19128076.6866\n",
      "Epoch 2/5\n",
      "1159686/1159686 [==============================] - 138s 119us/step - loss: 4034421448.9236 - mean_absolute_error: 778.0972 - mean_squared_error: 4034421448.9236 - val_loss: 17279130.7840 - val_mean_absolute_error: 700.3675 - val_mean_squared_error: 17279130.7840\n",
      "Epoch 3/5\n",
      "1159686/1159686 [==============================] - 138s 119us/step - loss: 4032668884.5384 - mean_absolute_error: 802.5082 - mean_squared_error: 4032668884.5384 - val_loss: 16411659.1315 - val_mean_absolute_error: 683.9459 - val_mean_squared_error: 16411659.1315\n",
      "Epoch 4/5\n",
      "1159686/1159686 [==============================] - 137s 119us/step - loss: 4031522688.3064 - mean_absolute_error: 849.8324 - mean_squared_error: 4031522688.3064 - val_loss: 16118538.7335 - val_mean_absolute_error: 694.5299 - val_mean_squared_error: 16118538.7335\n",
      "Epoch 5/5\n",
      "1159686/1159686 [==============================] - 138s 119us/step - loss: 4030509609.3292 - mean_absolute_error: 866.7567 - mean_squared_error: 4030509609.3292 - val_loss: 16018091.1721 - val_mean_absolute_error: 700.8584 - val_mean_squared_error: 16018091.1721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9326d0b940>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "gc.collect()\n",
    "# set parameters:\n",
    "max_features = 50000\n",
    "maxlen = 100\n",
    "batch_size = 2048\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 5\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "X,X_gl,y = df_train['text_encoded'].values,df_train[gl_cols],df_train['PRICE'].values\n",
    "x_train, x_test,x_gl_train,x_gl_test, y_train, y_test = train_test_split(X,X_gl, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "main_input = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen)(main_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "\n",
    "x = Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1)(x)\n",
    "\n",
    "x1 = MaxPooling1D(pool_size=2)(x)\n",
    "x2 = AveragePooling1D(pool_size=2)(x)\n",
    "x = concatenate([x1,x2])\n",
    "\n",
    "xp = x\n",
    "x = transition_layer(x, n_kernels=32,dropout=0)\n",
    "x = conv_layer(x, n_kernels=64,kernel_size=3,dilation_rate=1,padding='same')\n",
    "x = concatenate([x,xp])\n",
    "# we use max pooling:\n",
    "x2 = GlobalAveragePooling1D()(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = concatenate([x,x2])\n",
    "\n",
    "auxiliary_input =  Input(shape=(len(gl_cols),), dtype='float32', name='aux_input')\n",
    "x = concatenate([x,auxiliary_input])\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = Dense(hidden_dims)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "main_output = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output])\n",
    "\n",
    "optimizer = Adam(lr=0.0001,)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mae','mse'])\n",
    "model.count_params()\n",
    "\n",
    "model.fit([x_train,x_gl_train], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=([x_test,x_gl_test], y_test),shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "2772787\n",
    "Train on 1159686 samples, validate on 289922 samples\n",
    "Epoch 1/5\n",
    "1159686/1159686 [==============================] - 177s 152us/step - loss: 4035862812.1876 - mean_absolute_error: 918.4204 - mean_squared_error: 4035862812.1876 - val_loss: 17763249.4091 - val_mean_absolute_error: 738.8896 - val_mean_squared_error: 17763249.4091\n",
    "Epoch 2/5\n",
    "1159686/1159686 [==============================] - 159s 137us/step - loss: 4033106637.6809 - mean_absolute_error: 819.1301 - mean_squared_error: 4033106637.6809 - val_loss: 16593876.7654 - val_mean_absolute_error: 692.8858 - val_mean_squared_error: 16593876.7654\n",
    "Epoch 3/5\n",
    "1159686/1159686 [==============================] - 159s 137us/step - loss: 4031885271.2714 - mean_absolute_error: 807.6185 - mean_squared_error: 4031885271.2714 - val_loss: 16187226.2574 - val_mean_absolute_error: 689.4169 - val_mean_squared_error: 16187226.2574\n",
    "Epoch 4/5\n",
    "1159686/1159686 [==============================] - 159s 137us/step - loss: 4030434700.2492 - mean_absolute_error: 800.9113 - mean_squared_error: 4030434700.2492 - val_loss: 16142123.4253 - val_mean_absolute_error: 686.3089 - val_mean_squared_error: 16142123.4253\n",
    "Epoch 5/5\n",
    "1159686/1159686 [==============================] - 159s 137us/step - loss: 4028943375.4024 - mean_absolute_error: 779.0099 - mean_squared_error: 4028943375.4024 - val_loss: 17481870.5746 - val_mean_absolute_error: 1469.6001 - val_mean_squared_error: 17481870.5746\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "X_test = df_test['text_encoded'].values\n",
    "X_test_gl = df_test[gl_cols]\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "y_preds = model.predict([X_test,X_test_gl])\n",
    "\n",
    "df_results = df_test[['ID']]\n",
    "df_results['PRICE'] = y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1585751</td>\n",
       "      <td>289.457550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1530678</td>\n",
       "      <td>748.071350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1324955</td>\n",
       "      <td>289.457550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>822454</td>\n",
       "      <td>1509.970459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1604015</td>\n",
       "      <td>625.523804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID        PRICE\n",
       "0  1585751   289.457550\n",
       "1  1530678   748.071350\n",
       "2  1324955   289.457550\n",
       "3   822454  1509.970459\n",
       "4  1604015   625.523804"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.head()\n",
    "df_results.to_csv(\"baseline-3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2282"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2793203"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1159686 samples, validate on 289922 samples\n",
      "Epoch 1/10\n",
      "1159686/1159686 [==============================] - 225s 194us/step - loss: 4035866176.8800 - mean_absolute_error: 913.9113 - mean_squared_error: 4035866176.8800 - val_loss: 17918867.4867 - val_mean_absolute_error: 753.0236 - val_mean_squared_error: 17918867.4867\n",
      "Epoch 2/10\n",
      "1159686/1159686 [==============================] - 208s 180us/step - loss: 4032961440.3657 - mean_absolute_error: 807.5361 - mean_squared_error: 4032961440.3657 - val_loss: 16587690.5692 - val_mean_absolute_error: 694.2264 - val_mean_squared_error: 16587690.5692\n",
      "Epoch 3/10\n",
      "1159686/1159686 [==============================] - 208s 180us/step - loss: 4030689757.3081 - mean_absolute_error: 817.8499 - mean_squared_error: 4030689757.3081 - val_loss: 16222575.0565 - val_mean_absolute_error: 692.6056 - val_mean_squared_error: 16222575.0565\n",
      "Epoch 4/10\n",
      "1159686/1159686 [==============================] - 208s 180us/step - loss: 4028827278.6527 - mean_absolute_error: 825.5131 - mean_squared_error: 4028827278.6527 - val_loss: 16277692.7971 - val_mean_absolute_error: 693.2791 - val_mean_squared_error: 16277692.7971\n",
      "Epoch 5/10\n",
      "1159686/1159686 [==============================] - 208s 180us/step - loss: 4025637375.7932 - mean_absolute_error: 827.0715 - mean_squared_error: 4025637375.7932 - val_loss: 16537363.8846 - val_mean_absolute_error: 734.5397 - val_mean_squared_error: 16537363.8846\n",
      "Epoch 6/10\n",
      "1159686/1159686 [==============================] - 208s 180us/step - loss: 4021088772.0070 - mean_absolute_error: 826.7528 - mean_squared_error: 4021088772.0070 - val_loss: 16958375.0062 - val_mean_absolute_error: 747.0858 - val_mean_squared_error: 16958375.0062\n",
      "Epoch 7/10\n",
      "1159686/1159686 [==============================] - 208s 180us/step - loss: 4021654669.3745 - mean_absolute_error: 841.0682 - mean_squared_error: 4021654669.3745 - val_loss: 16845657.6819 - val_mean_absolute_error: 716.6564 - val_mean_squared_error: 16845657.6819\n",
      "Epoch 8/10\n",
      "1159686/1159686 [==============================] - 208s 180us/step - loss: 4018156112.6231 - mean_absolute_error: 838.9194 - mean_squared_error: 4018156112.6231 - val_loss: 17123202.7639 - val_mean_absolute_error: 791.7502 - val_mean_squared_error: 17123202.7639\n",
      "Epoch 9/10\n",
      "1159686/1159686 [==============================] - 208s 179us/step - loss: 4015867325.2139 - mean_absolute_error: 842.8016 - mean_squared_error: 4015867325.2139 - val_loss: 17399316.6838 - val_mean_absolute_error: 711.0922 - val_mean_squared_error: 17399316.6838\n",
      "Epoch 10/10\n",
      "1159686/1159686 [==============================] - 208s 179us/step - loss: 4015413438.6439 - mean_absolute_error: 805.1221 - mean_squared_error: 4015413438.6439 - val_loss: 17799727.6086 - val_mean_absolute_error: 890.6861 - val_mean_squared_error: 17799727.6086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9323662a58>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "gc.collect()\n",
    "# set parameters:\n",
    "max_features = 50000\n",
    "maxlen = 100\n",
    "batch_size = 2048\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "X,X_gl,y = df_train['text_encoded'].values,df_train[gl_cols],df_train['PRICE'].values\n",
    "x_train, x_test,x_gl_train,x_gl_test, y_train, y_test = train_test_split(X,X_gl, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "main_input = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen)(main_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "\n",
    "x = Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1)(x)\n",
    "xp = x\n",
    "x = transition_layer(x, n_kernels=64,dropout=0)\n",
    "xp2 = conv_layer(x,n_kernels=64,kernel_size=3,dilation_rate=1,padding='same')\n",
    "\n",
    "x = concatenate([xp2, xp])\n",
    "x = transition_layer(x, n_kernels=64,dropout=0)\n",
    "xp3 = conv_layer(x,n_kernels=64,kernel_size=3,dilation_rate=2,padding='same')\n",
    "\n",
    "x = concatenate([xp,xp2,xp3])\n",
    "# we use max pooling:\n",
    "x2 = GlobalAveragePooling1D()(x)\n",
    "x1 = GlobalMaxPooling1D()(x)\n",
    "x = concatenate([x1,x2])\n",
    "\n",
    "auxiliary_input =  Input(shape=(len(gl_cols),), dtype='float32', name='aux_input')\n",
    "x = concatenate([x,auxiliary_input])\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "x = Dense(hidden_dims)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "main_output = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output])\n",
    "\n",
    "optimizer = Adam(lr=0.0001,)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mae','mse'])\n",
    "model.count_params()\n",
    "\n",
    "model.fit([x_train,x_gl_train], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=([x_test,x_gl_test], y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 128\n",
    "maxlen = 500\n",
    "batch_size = 1024\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "X,X_gl,y = df_train['char_encoded'].values,df_train[gl_cols],df_train['PRICE'].values\n",
    "x_train, x_test,x_gl_train,x_gl_test, y_train, y_test = train_test_split(X,X_gl, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "main_input = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(output_dim=embedding_dims, input_dim=max_features, input_length=maxlen)(main_input)\n",
    "x1 = MaxPooling1D(pool_size=5)(x)\n",
    "x2 = AveragePooling1D(pool_size=5)(x)\n",
    "x = concatenate([x1,x2])\n",
    "\n",
    "x = conv_layer(x,n_kernels=128,kernel_size=5, padding='valid')\n",
    "# x1 = MaxPooling1D(pool_size=2)(x)\n",
    "# x2 = AveragePooling1D(pool_size=2)(x)\n",
    "# x = concatenate([x1,x2])\n",
    "xp1 = x\n",
    "x = transition_layer(x, n_kernels=64, dropout=0)\n",
    "\n",
    "x = conv_layer(x,n_kernels=64,kernel_size=15,padding='same')\n",
    "xp2 = x\n",
    "\n",
    "x = concatenate([x,xp1])\n",
    "x = transition_layer(x, n_kernels=32, dropout=0)\n",
    "x = conv_layer(x,n_kernels=32,kernel_size=15,dilation_rate=2,padding='same')\n",
    "xp3 = x\n",
    "\n",
    "x = concatenate([xp2,x])\n",
    "x = transition_layer(x, n_kernels=32,dropout=0)\n",
    "x = conv_layer(x,n_kernels=64,kernel_size=15,dilation_rate=1,padding='same')\n",
    "x = transition_layer(x, n_kernels=32, dropout=0)\n",
    "\n",
    "x = concatenate([x,xp1,xp3])\n",
    "\n",
    "x = transition_layer(x, n_kernels=64,dropout=0)\n",
    "\n",
    "\n",
    "x = pre_dense_layer(x)\n",
    "K.int_shape(x)\n",
    "auxiliary_input =  Input(shape=(len(gl_cols),), dtype='float32', name='aux_input')\n",
    "x_aux = Dense(4)(auxiliary_input)\n",
    "x = concatenate([x,x_aux])\n",
    "x = Dense(16)(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x =Activation('relu')(x)\n",
    "main_output = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output])\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model.hdf5\", monitor='mse', verbose=0, save_best_only=True, mode='max')\n",
    "\n",
    "optimizer = Adam(0.0001)\n",
    "\n",
    "callbacks_list = []\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mae','mse'])\n",
    "print(\"Params = \",model.count_params())\n",
    "\n",
    "model.fit([x_train,x_gl_train], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,callbacks=callbacks_list,\n",
    "          validation_data=([x_test,x_gl_test], y_test),\n",
    "         shuffle=True)\n",
    "\n",
    "\n",
    "# model.load_weights(\"model.hdf5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Params =  134401\n",
    "Train on 1159686 samples, validate on 289922 samples\n",
    "Epoch 1/5\n",
    "1159686/1159686 [==============================] - 132s 114us/step - loss: 4037083525.2508 - mean_absolute_error: 1044.8966 - mean_squared_error: 4037083525.2508 - val_loss: 19944968.5354 - val_mean_absolute_error: 887.9338 - val_mean_squared_error: 19944968.5354\n",
    "Epoch 2/5\n",
    "1159686/1159686 [==============================] - 120s 103us/step - loss: 4036030117.0389 - mean_absolute_error: 873.6337 - mean_squared_error: 4036030117.0389 - val_loss: 18914763.4832 - val_mean_absolute_error: 759.5289 - val_mean_squared_error: 18914763.4832\n",
    "Epoch 3/5\n",
    "1159686/1159686 [==============================] - 120s 103us/step - loss: 4035054825.3241 - mean_absolute_error: 855.0202 - mean_squared_error: 4035054825.3241 - val_loss: 18379962.2891 - val_mean_absolute_error: 751.8860 - val_mean_squared_error: 18379962.2891\n",
    "Epoch 4/5\n",
    "1159686/1159686 [==============================] - 120s 103us/step - loss: 4034294206.2672 - mean_absolute_error: 907.1308 - mean_squared_error: 4034294206.2672 - val_loss: 18337784.9426 - val_mean_absolute_error: 802.2734 - val_mean_squared_error: 18337784.9426\n",
    "Epoch 5/5\n",
    "1159686/1159686 [==============================] - 120s 103us/step - loss: 4031880473.4394 - mean_absolute_error: 926.9110 - mean_squared_error: 4031880473.4394 - val_loss: 23768072.0290 - val_mean_absolute_error: 1974.2186 - val_mean_squared_error: 23768072.0290\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing\n",
    "- gl min max clipping\n",
    "- max(0,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
