{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps I followed\n",
    "- Understand your data, check distributions\n",
    "- Create a good Baseline\n",
    "- Improve using Model\n",
    "- Inspect Model\n",
    "- Iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T16:59:43.251163Z",
     "start_time": "2019-06-14T16:59:43.200912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lib' from '/Users/ahemf/Desktop/ML_hackathon/lib.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np_utils\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, DepthwiseConv2D, Conv2D, SeparableConv2D, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers import Input, concatenate, LeakyReLU\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Nadam, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.regularizers import l2\n",
    "%config InlineBackend.figure_format='retina'\n",
    "from keras_contrib.callbacks import CyclicLR\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from data_science_utils.vision.keras import *\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import missingno as msno\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "from data_science_utils import dataframe as df_utils\n",
    "from data_science_utils import models as model_utils\n",
    "from data_science_utils import plots as plot_utils\n",
    "from data_science_utils.dataframe import column as column_utils\n",
    "from data_science_utils import misc as misc\n",
    "from data_science_utils import preprocessing as pp_utils\n",
    "from data_science_utils import nlp as nlp_utils\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from data_science_utils.dataframe import get_specific_cols\n",
    "\n",
    "import more_itertools\n",
    "from more_itertools import flatten\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "from importlib import reload\n",
    "import lib\n",
    "reload(lib)\n",
    "from lib import *\n",
    "\n",
    "from oclr import OneCycleLR, LRFinder\n",
    "from data_science_utils.models import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"price_prediction/training.csv\")\n",
    "df_test = pd.read_csv(\"price_prediction/public_test_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "- highlight `replace_numbers` and `clean_text`\n",
    "- highlight `word lemmatization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = preprocess_for_word_cnn(df_train,jobs=32)\n",
    "gl_le_train,gl_le_transform, le = get_text_le(\"text\")\n",
    "_ = gl_le_train(df_train)\n",
    "df_train['text_encoded'] = gl_le_transform(df_train)\n",
    "\n",
    "df_test = preprocess_for_word_cnn(df_test,jobs=32)\n",
    "df_test['text_encoded'] = gl_le_transform(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = preprocess_for_char_cnn(df_train,jobs=32)\n",
    "df_test = preprocess_for_char_cnn(df_test,jobs=32)\n",
    "char_le_train,char_le_transform, char_le = get_char_le(\"char\")\n",
    "_ = char_le_train(df_train)\n",
    "\n",
    "df_train['char_encoded'] = char_le_transform(df_train)\n",
    "df_test['char_encoded'] = char_le_transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test[[\"ID\",\"GL\",\"text\",\"text_encoded\",\"char\",\"char_encoded\"]].to_csv(\"price_prediction/test.csv\",index=False)\n",
    "# df_train[[\"ID\",\"GL\",\"text\",\"text_encoded\",\"char\",\"char_encoded\",\"PRICE\"]].to_csv(\"price_prediction/train.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GL Based baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.GL.nunique()\n",
    "df_test.GL.nunique()\n",
    "\n",
    "set(df_test.GL.unique()) - set(df_train.GL.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gl_means = df_train.groupby([\"GL\"])[['PRICE']].mean().reset_index()\n",
    "\n",
    "df_results = df_test.merge(df_gl_means, on=[\"GL\"],how=\"left\")\n",
    "df_results = df_results[[\"ID\",\"PRICE\"]]\n",
    "df_results[\"PRICE\"] = df_results[\"PRICE\"].fillna(df_results[\"PRICE\"].mean())\n",
    "\n",
    "df_results.head()\n",
    "\n",
    "df_results.to_csv(\"baseline.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GL level pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext commandline based baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ft = preprocess_for_fasttext_cmd(df_train.copy(),jobs=32)\n",
    "df_test_ft = preprocess_for_fasttext_cmd(df_test.copy(),jobs=32)\n",
    "\n",
    "df_train['RPRICE'] = np.ceil(np.sqrt(df_train['PRICE']))\n",
    "df_train['label'] = '__label__'\n",
    "df_train['label'] = df_train['label'] + df_train['RPRICE'].astype(str)\n",
    "\n",
    "df_train['text'] = df_train['label']\n",
    "df_train['text'] = df_train['text'] + \" \"\n",
    "df_train['text'] = df_train['text'] + df_train['char']\n",
    "\n",
    "df_train[df_train['RPRICE']>=80][[\"text\",\"price\"]].sample(10)\n",
    "df_test['text'] = df_test['char']\n",
    "\n",
    "train,test = train_test_split(df_train[['text']],test_size=0.2, random_state=42)\n",
    "train[['text']].to_csv(\"fastText-0.2.0/train-1.txt\",header=False,index=False)\n",
    "test[['text']].to_csv(\"fastText-0.2.0/train-2.txt\",header=False,index=False)\n",
    "\n",
    "df_train[['text']].to_csv(\"fastText-0.2.0/train.txt\",header=False,index=False)\n",
    "df_test[['text']].to_csv(\"fastText-0.2.0/test.txt\",header=False,index=False)\n",
    "\n",
    "!head -n2 fastText-0.2.0/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv(\"fastText-0.2.0/prediction.txt\",header=None)\n",
    "\n",
    "df_results.shape\n",
    "df_results.columns=[\"result\"]\n",
    "df_results.head()\n",
    "\n",
    "df_results['values'] = df_results['result'].apply(lambda x:x[9:]).astype(float)\n",
    "df_results['values'] = np.power(df_results['values'],3)\n",
    "df_results.head()\n",
    "\n",
    "df_sub = df_test[['ID']]\n",
    "df_sub['PRICE'] = df_results['values']\n",
    "df_sub.to_csv(\"fasttext-cubic.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "- We use embedding dimension of 50 through-out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to 1D CNNs and Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Analogues**\n",
    "- 1 image = 1 Asin Full Text\n",
    "- Image Channels = Embedding Dimensions\n",
    "- Images are 2D (depth is 3 channels), Text is 1D sequence (depth is 50 embedding dimensions)\n",
    "- Edges/Gradients/Patterns in images = Text Phrases and important multi-word sequences\n",
    "\n",
    "**Understanding the structure of each row**\n",
    "\n",
    "Initially we have : Words -> Full text\n",
    "\n",
    "Finally after embedding we get : Each word as a list of 50 numbers, Text as list of Words. \n",
    "\n",
    "Single row shape is `(1,50,#Words)`\n",
    "\n",
    "Full Data shape is `(#Num examples, 50, #Word_Per_Row)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winning Model: Using Words and Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models\n",
    "\n",
    "- Model 2: Using Characters and Embedding Layer\n",
    "    - Very Less Preprocessing needed since used set of characters is around 128.\n",
    "    - Since the Sequence length is very long this is harder to tune though.\n",
    "- Model 3: Using Pretrained Glove-twitter-50 Embeddings\n",
    "- Model 4: The Fallen Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Embeddings\n",
    "\n",
    "- Word Cloud and hue on price bucket (Select top 10 words from each bucket)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Params and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Image representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T15:25:18.212873Z",
     "start_time": "2019-06-14T15:25:18.208338Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ensemble's image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Do Costly vs Cheaper Asins Look in Images\n",
    "\n",
    "- We use a 3 channel intermediate layer and use it here\n",
    "- We check 5 images from each price bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Inspection\n",
    "\n",
    "- We inspect top 10 Asins by RMSE, MAE, MAPE to see if we can change our preprocessing style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations and Suggestion for Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finding Good Learning rate was important\n",
    "- Model Trained Embeddings did better than Pretrained Embeddings (Data size may be a reason)\n",
    "- Going Wide Helped\n",
    "- P3.8x Large Sagemaker Instance has multi-gpu (8 total), this needs separate Tensorflow sessions, I used separate notebooks\n",
    "- Normalizing word vectors for Pretrained model was lowering performance, word vector lengths matter when you consider Text classification tasks, for NLP tasks like similarity they don't since cosine distance is used.\n",
    "- The Process of using Word CNNs is easily carried over to Sub-categories Classification\n",
    "- Why Ensembling Did not work?\n",
    "    - Models with dropouts are like ensembles themselves\n",
    "    - Only significant architecture or preprocessing step difference will help\n",
    "- Why Char CNN did not work?\n",
    "    - Long training time and huge number of computations\n",
    "- Following a structured process was important\n",
    "    - You might think something will improve the results but trying things without reasoning usually gives sub-par results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T16:00:15.106694Z",
     "start_time": "2019-06-14T16:00:15.103192Z"
    }
   },
   "outputs": [],
   "source": [
    "# Target Distribution Histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training with Log/CubeRoot/Standard Scaling of target since target is skewed.\n",
    "- Higher Embedding Dimensions\n",
    "- Pretrained Fasttext or Bert/Elmo Models for text representation\n",
    "- Inspecting Errors and finding text patterns we missed\n",
    "- Trying POS tags with lemmatized words\n",
    "- Bucketing/Quantizing: Dividing prices into cube-rooted buckets and then feeding the top 10 bucket prediction to final model.\n",
    "- LR scheduling, Cyclic LR and Super Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asin Classification into Sub-Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
