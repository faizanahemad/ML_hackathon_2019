{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T10:56:06.982560Z",
     "start_time": "2019-06-12T10:56:06.943268Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np_utils\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, DepthwiseConv2D, Conv2D, SeparableConv2D, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers import Input, concatenate, LeakyReLU\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Nadam, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.regularizers import l2\n",
    "%config InlineBackend.figure_format='retina'\n",
    "from keras_contrib.callbacks import CyclicLR\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from data_science_utils.vision.keras import *\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import missingno as msno\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "from data_science_utils import dataframe as df_utils\n",
    "from data_science_utils import models as model_utils\n",
    "from data_science_utils import plots as plot_utils\n",
    "from data_science_utils.dataframe import column as column_utils\n",
    "from data_science_utils import misc as misc\n",
    "from data_science_utils import preprocessing as pp_utils\n",
    "from data_science_utils import nlp as nlp_utils\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from data_science_utils.dataframe import get_specific_cols\n",
    "\n",
    "import more_itertools\n",
    "from more_itertools import flatten\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lib' from '/home/ec2-user/SageMaker/ML_hackathon_2019/lib.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "from importlib import reload\n",
    "import lib\n",
    "reload(lib)\n",
    "from lib import *\n",
    "\n",
    "from oclr import OneCycleLR, LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T09:42:02.848654Z",
     "start_time": "2019-06-12T09:41:52.143642Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"price_prediction/train.csv\")\n",
    "df_test = pd.read_csv(\"price_prediction/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f5ea2025364984b0c69fd0bf511524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1449608), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600bd3258f6943e6a8f28a3f449424c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1449608), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3888b15d15bb4b1898c047e24730eb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1449608), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8d1f287c6546d2a254c6870e7b045f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=362403), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672a1e2265004fe2bc0c75dabd8e9ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=362403), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc345998a764c1b87ff814bb0fc1515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=362403), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_train['text'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_train['text']))\n",
    "df_train['text_encoded'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_train['text_encoded']))\n",
    "df_train['char_encoded'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_train['char_encoded']))\n",
    "\n",
    "df_test['text'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_test['text']))\n",
    "df_test['text_encoded'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_test['text_encoded']))\n",
    "df_test['char_encoded'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_test['char_encoded']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GL encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Categorical fit start at: 2019-06-13 20:13:23.712711\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Shape of Input to Neural Network: (61, 61), Output shape: (61, 69)\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Categorical fit done at: 2019-06-13 20:13:41.422190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<data_science_utils.preprocessing.NeuralCategoricalFeatureTransformer at 0x7f1b8c502908>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_science_utils.preprocessing import NeuralCategoricalFeatureTransformer\n",
    "\n",
    "ct_nn = NeuralCategoricalFeatureTransformer(cols=[\"GL\"],prefix=\"gl_encoded_\",\n",
    "                                            target_columns=[\"PRICE\"],verbose=0,n_components=16,n_iter=200,)\n",
    "\n",
    "ct_nn.fit(df_train)\n",
    "\n",
    "ct_nn.skip_fit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = ct_nn.transform(df_train)\n",
    "df_test = ct_nn.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_cols = get_specific_cols(df_train,prefix='gl_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total =  55.031386852264404\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "glove = api.load(\"glove-twitter-25\") \n",
    "print(\"total = \",(time()-start))\n",
    "\n",
    "ptr_glove = PreTrainedEmbeddingsTransformer(glove,size=25)\n",
    "ptr_glove.fit()\n",
    "\n",
    "df_test['glove_encoded'] = ptr_glove.transform(df_test['text'].values)\n",
    "df_train['glove_encoded'] = ptr_glove.transform(df_train['text'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total =  74.3492660522461\n",
      "Fasttext Transforms start at: 2019-06-13 20:15:43.567292\n",
      "Number of Unique Test Tokens for Fasttext transform 160522\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d8a70fbc8a441d89fc64122704d1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=362403), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fasttext Transforms done at: 2019-06-13 20:16:00.514933\n",
      "Fasttext Transforms start at: 2019-06-13 20:16:00.533301\n",
      "Number of Unique Test Tokens for Fasttext transform 332242\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7deb2cfa844bcd98f84189f3e3e7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1449608), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fasttext Transforms done at: 2019-06-13 20:16:34.420065\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "glove = api.load(\"glove-twitter-50\") \n",
    "print(\"total = \",(time()-start))\n",
    "\n",
    "ptr_glove_50 = PreTrainedEmbeddingsTransformer(glove,size=50)\n",
    "ptr_glove_50.fit()\n",
    "\n",
    "df_test['glove_encoded-50'] = ptr_glove_50.transform(df_test['text'].values)\n",
    "df_train['glove_encoded-50'] = ptr_glove_50.transform(df_train['text'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "fasttext = api.load(\"fasttext-wiki-news-subwords-300\") \n",
    "print(\"total = \",(time()-start))\n",
    "\n",
    "ptr = PreTrainedEmbeddingsTransformer(fasttext,size=300)\n",
    "ptr.fit()\n",
    "\n",
    "df_train['fasttext_encoded'] = ptr.transform(df_train['text'].values)\n",
    "df_test['fasttext_encoded'] = ptr.transform(df_test['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "max_char_features = 128\n",
    "char_maxlen = 500\n",
    "char_embedding_dims = 20\n",
    "\n",
    "text_max_features = 50000\n",
    "text_maxlen = 100\n",
    "text_embedding_dims = 25\n",
    "\n",
    "enc_maxlen = 100\n",
    "enc_embedding_dims = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "X_enc,X_text,X_char,X_gl,y = df_train['glove_encoded-50'].values,df_train['text_encoded'].values,df_train['char_encoded'].values,df_train[gl_cols],df_train['PRICE'].values\n",
    "x_train_enc, x_test_enc,x_train_text, x_test_text, x_train_char, x_test_char,x_gl_train,x_gl_test, y_train, y_test = train_test_split(X_enc,X_text,X_char,X_gl, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train_enc = sequence.pad_sequences(x_train_enc, maxlen=enc_maxlen)\n",
    "x_test_enc = sequence.pad_sequences(x_test_enc, maxlen=enc_maxlen)\n",
    "\n",
    "x_train_text = sequence.pad_sequences(x_train_text, maxlen=text_maxlen)\n",
    "x_test_text = sequence.pad_sequences(x_test_text, maxlen=text_maxlen)\n",
    "\n",
    "x_train_char = sequence.pad_sequences(x_train_char, maxlen=char_maxlen)\n",
    "x_test_char = sequence.pad_sequences(x_test_char, maxlen=char_maxlen)\n",
    "\n",
    "x_train_enc = x_train_enc.reshape((-1,100,enc_embedding_dims,1))\n",
    "x_test_enc = x_test_enc.reshape((-1,100,enc_embedding_dims,1))\n",
    "\n",
    "datagen = ImageDataGenerator(featurewise_center=True,featurewise_std_normalization=True,)\n",
    "datagen.fit(x_train_enc)\n",
    "train_iterator = datagen.flow(x_train_enc, y_train, batch_size=len(x_train_enc),shuffle=True)\n",
    "validation_iterator = datagen.flow(x_test_enc, y_test, batch_size=len(x_test_enc),shuffle=True)\n",
    "\n",
    "x_train_enc = train_iterator.next()[0]\n",
    "x_test_enc = validation_iterator.next()[0]\n",
    "\n",
    "x_train_enc = x_train_enc.reshape((-1,100,enc_embedding_dims))\n",
    "x_test_enc = x_test_enc.reshape((-1,100,enc_embedding_dims))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_joiner(intermediates):\n",
    "    x = concatenate(intermediates)\n",
    "    x = transition_layer(x, n_kernels=64,dropout=0)\n",
    "    x1 = conv_layer(x,n_kernels=64,kernel_size=3,padding='same')\n",
    "    x2 = conv_layer(x1,n_kernels=128,kernel_size=3,padding='same')\n",
    "    x = concatenate([x,x1,x2])\n",
    "    x = transition_layer(x, n_kernels=64, dropout=0)\n",
    "    \n",
    "    x1 = AveragePooling1D()(x) \n",
    "    x2 = MaxPooling1D()(x)\n",
    "    x = concatenate([x1,x2])\n",
    "    x = transition_layer(x, n_kernels=64, dropout=0)\n",
    "    x = conv_layer(x,n_kernels=128,kernel_size=3,padding='same')\n",
    "    \n",
    "    print(\"Before FC Intermediate =\",K.int_shape(x))\n",
    "    out1 = GlobalAveragePooling1D()(x)\n",
    "    out2 = GlobalMaxPooling1D()(x)\n",
    "    out = concatenate([out1,out2])\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_output_fcnn(inputs):\n",
    "    x = Dense(16)(inputs)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Dense(1)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cnn(inputs):\n",
    "    x = inputs\n",
    "    filters = 250\n",
    "    kernel_size = 3\n",
    "    x = Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1)(x)\n",
    "\n",
    "    x1 = MaxPooling1D(pool_size=2)(x)\n",
    "    x2 = AveragePooling1D(pool_size=2)(x)\n",
    "    x = concatenate([x1,x2])\n",
    "\n",
    "    xp = x\n",
    "    x = transition_layer(x, n_kernels=32,dropout=0)\n",
    "    x = conv_layer(x, n_kernels=64,kernel_size=3,dilation_rate=1,padding='same')\n",
    "    x = concatenate([x,xp])\n",
    "    x = transition_layer(x, n_kernels=128,dropout=0)\n",
    "    intermidate = x\n",
    "    # we use max pooling:\n",
    "    x2 = GlobalAveragePooling1D()(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([x,x2])\n",
    "    \n",
    "    print(\"W1 Intermediate =\",K.int_shape(intermidate))\n",
    "    return intermidate, aux_output_fcnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cnn_v2(inputs):\n",
    "    \n",
    "    filters = 250\n",
    "    kernel_size = 3\n",
    "    x = inputs\n",
    "    x = Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1)(x)\n",
    "    \n",
    "    x1 = MaxPooling1D(pool_size=2)(x)\n",
    "    x2 = AveragePooling1D(pool_size=2)(x)\n",
    "    x = concatenate([x1,x2])\n",
    "    \n",
    "    xp = x\n",
    "    x = transition_layer(x, n_kernels=64,dropout=0)\n",
    "    xp2 = conv_layer(x,n_kernels=64,kernel_size=3,dilation_rate=1,padding='same')\n",
    "\n",
    "    x = concatenate([xp2, xp])\n",
    "    x = transition_layer(x, n_kernels=64,dropout=0)\n",
    "    xp3 = conv_layer(x,n_kernels=64,kernel_size=3,dilation_rate=2,padding='same')\n",
    "\n",
    "    x = concatenate([xp,xp2,xp3])\n",
    "    x = transition_layer(x, n_kernels=128,dropout=0)\n",
    "    intermidate = x\n",
    "    # we use max pooling:\n",
    "    x2 = GlobalAveragePooling1D()(x)\n",
    "    x1 = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([x1,x2])\n",
    "    print(\"W2 Intermediate =\",K.int_shape(intermidate))\n",
    "    return intermidate, aux_output_fcnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_cnn():\n",
    "    main_input = Input(shape=(char_maxlen,), dtype='int32',)\n",
    "    x = Embedding(output_dim=char_embedding_dims, input_dim=max_char_features, input_length=char_maxlen)(main_input)\n",
    "    \n",
    "    x1 = MaxPooling1D(pool_size=5)(x)\n",
    "    x2 = AveragePooling1D(pool_size=5)(x)\n",
    "    x = concatenate([x1,x2])\n",
    "    \n",
    "    x = conv_layer(x,n_kernels=64, kernel_size=3,padding='valid')\n",
    "    \n",
    "    x1 = MaxPooling1D(pool_size=2)(x)\n",
    "    x2 = AveragePooling1D(pool_size=2)(x)\n",
    "    x = concatenate([x1,x2])\n",
    "    \n",
    "    xp1 = x\n",
    "    \n",
    "    x = transition_layer(x, n_kernels=32, dropout=0)\n",
    "    x = conv_layer(x,n_kernels=64,kernel_size=15,padding='same')\n",
    "    xp2 = x\n",
    "    x = concatenate([x,xp1])\n",
    "    \n",
    "    x = transition_layer(x, n_kernels=32, dropout=0)\n",
    "    x = conv_layer(x,n_kernels=64,kernel_size=15,padding='same', dilation_rate=2)\n",
    "    x = concatenate([x,xp2])\n",
    "    intermidate = x\n",
    "    x2 = GlobalAveragePooling1D()(x)\n",
    "    x1 = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([x1,x2])\n",
    "    print(\"Char Cnn Intermediate =\",K.int_shape(intermidate))\n",
    "    return main_input, intermidate, aux_output_fcnn(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_cnn():\n",
    "    main_input = Input(shape=(enc_maxlen,enc_embedding_dims), dtype='float32')\n",
    "    x = main_input\n",
    "    x = conv_layer(x,n_kernels=64,kernel_size=3,padding='valid')\n",
    "    \n",
    "    x1 = MaxPooling1D(pool_size=2)(x)\n",
    "    x2 = AveragePooling1D(pool_size=2)(x)\n",
    "    x = concatenate([x1,x2])\n",
    "    \n",
    "    xp = x\n",
    "    x = transition_layer(x, n_kernels=32,dropout=0)\n",
    "    x = conv_layer(x,n_kernels=64,kernel_size=3,padding='same')\n",
    "    x = transition_layer(x, n_kernels=32,dropout=0)\n",
    "\n",
    "    xp2 = x\n",
    "    x = concatenate([x,xp])\n",
    "    x = transition_layer(x, n_kernels=32,dropout=0)\n",
    "    x = conv_layer(x,n_kernels=64,kernel_size=3,dilation_rate=2,padding='same')\n",
    "    x = transition_layer(x, n_kernels=32,dropout=0)\n",
    "\n",
    "    x = concatenate([x,xp,xp2])\n",
    "\n",
    "    x = transition_layer(x, n_kernels=128,dropout=0)\n",
    "    intermidate = x\n",
    "    print(\"Pretrained Intermediate =\",K.int_shape(intermidate))\n",
    "    x = pre_dense_layer(x)\n",
    "    return main_input,intermidate, aux_output_fcnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_layer(flattened,aux):\n",
    "    \n",
    "    x = concatenate([flattened,aux])\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = Dense(64)(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    output = Dense(1)(x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 Intermediate = (None, 49, 128)\n",
      "Char Cnn Intermediate = (None, 49, 128)\n",
      "Pretrained Intermediate = (None, 49, 128)\n",
      "Before FC Intermediate = (None, 24, 128)\n",
      "Params =  1652048\n"
     ]
    }
   ],
   "source": [
    "word_input = Input(shape=(text_maxlen,), dtype='int32')\n",
    "x = Embedding(text_max_features,\n",
    "            text_embedding_dims,\n",
    "            input_length=text_maxlen)(word_input)\n",
    "\n",
    "w1_intermidate, w1_output = word_cnn(x)\n",
    "\n",
    "c_inputs,c_intermidate, c_output = char_cnn()\n",
    "p_inputs,p_intermidate, p_output = pretrained_embedding_cnn()\n",
    "\n",
    "\n",
    "auxiliary_input =  Input(shape=(len(gl_cols),), dtype='float32', name='aux_input')\n",
    "x_aux = Dense(4)(auxiliary_input)\n",
    "\n",
    "intermediates = [w1_intermidate,c_intermidate,p_intermidate]\n",
    "x = intermediate_joiner(intermediates)\n",
    "\n",
    "op = final_layer(x,x_aux)\n",
    "\n",
    "outputs = [w1_output,c_output,p_output,op]\n",
    "inputs = [word_input,c_inputs,p_inputs,auxiliary_input]\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = Adam(lr=0.0001,)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mae'],loss_weights=[0.3,0.3,0.3,1.0])\n",
    "\n",
    "print(\"Params = \",model.count_params())\n",
    "# 1634125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1159686 samples, validate on 289922 samples\n",
      "Epoch 1/5\n",
      "1159686/1159686 [==============================] - 666s 574us/step - loss: 7666831663.5923 - dense_33_loss: 4035421708.7536 - dense_35_loss: 4036365429.8985 - dense_37_loss: 4036491918.9434 - dense_40_loss: 4034347805.8960 - dense_33_mean_absolute_error: 885.8146 - dense_35_mean_absolute_error: 959.4911 - dense_37_mean_absolute_error: 953.7178 - dense_40_mean_absolute_error: 872.0405 - val_loss: 33061327.8092 - val_dense_33_loss: 17309083.3915 - val_dense_35_loss: 19268256.5459 - val_dense_37_loss: 19497984.7625 - val_dense_40_loss: 16238730.1663 - val_dense_33_mean_absolute_error: 822.4257 - val_dense_35_mean_absolute_error: 879.6698 - val_dense_37_mean_absolute_error: 858.1819 - val_dense_40_mean_absolute_error: 916.6244\n",
      "Epoch 2/5\n",
      "1159686/1159686 [==============================] - 652s 562us/step - loss: 7663575750.5892 - dense_33_loss: 4033130301.4367 - dense_35_loss: 4035736284.7837 - dense_37_loss: 4036085803.0274 - dense_40_loss: 4032089755.1454 - dense_33_mean_absolute_error: 952.6243 - dense_35_mean_absolute_error: 1020.0262 - dense_37_mean_absolute_error: 1004.9498 - dense_40_mean_absolute_error: 870.4649 - val_loss: 32173146.2255 - val_dense_33_loss: 16530341.6882 - val_dense_35_loss: 19130525.0660 - val_dense_37_loss: 19477487.8861 - val_dense_40_loss: 15631639.1392 - val_dense_33_mean_absolute_error: 758.4122 - val_dense_35_mean_absolute_error: 851.9424 - val_dense_37_mean_absolute_error: 879.5819 - val_dense_40_mean_absolute_error: 691.0738\n",
      "Epoch 3/5\n",
      "1159680/1159686 [============================>.] - ETA: 0s - loss: 7660128186.9534 - dense_33_loss: 4031711878.9503 - dense_35_loss: 4035579708.2670 - dense_37_loss: 4036107746.2625 - dense_40_loss: 4029108293.4079 - dense_33_mean_absolute_error: 1036.7901 - dense_35_mean_absolute_error: 1015.8494 - dense_37_mean_absolute_error: 1024.6309 - dense_40_mean_absolute_error: 869.3375"
     ]
    }
   ],
   "source": [
    "model.fit([x_train_text,x_train_char,x_train_enc,x_gl_train], [y_train,y_train,y_train,y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=([x_test_text,x_test_char,x_test_enc,x_gl_test], [y_test,y_test,y_test,y_test]),\n",
    "         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
