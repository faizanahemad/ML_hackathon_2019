{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T10:56:06.982560Z",
     "start_time": "2019-06-12T10:56:06.943268Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import time\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np_utils\n",
    "%matplotlib inline\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, DepthwiseConv2D, Conv2D, SeparableConv2D, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers import Input, concatenate, LeakyReLU\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Nadam, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.regularizers import l2\n",
    "%config InlineBackend.figure_format='retina'\n",
    "from keras_contrib.callbacks import CyclicLR\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from data_science_utils.vision.keras import *\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import missingno as msno\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "from data_science_utils import dataframe as df_utils\n",
    "from data_science_utils import models as model_utils\n",
    "from data_science_utils import plots as plot_utils\n",
    "from data_science_utils.dataframe import column as column_utils\n",
    "from data_science_utils import misc as misc\n",
    "from data_science_utils import preprocessing as pp_utils\n",
    "from data_science_utils import nlp as nlp_utils\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from data_science_utils.dataframe import get_specific_cols\n",
    "\n",
    "import more_itertools\n",
    "from more_itertools import flatten\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lib' from '/home/ec2-user/SageMaker/ML_hackathon_2019/lib.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "from importlib import reload\n",
    "import lib\n",
    "reload(lib)\n",
    "from lib import *\n",
    "\n",
    "from oclr import OneCycleLR, LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T09:42:02.848654Z",
     "start_time": "2019-06-12T09:41:52.143642Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"asin_classification/train.csv\")\n",
    "df_test = pd.read_csv(\"asin_classification/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2315162, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(100000, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cca0b2051a4c5ba955eee5b3e1f545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2315162), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881eb2b804c249f881621b89eb83d5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2315162), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a96e8dc8244df2906db84f5112fe39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f270aa446244d3b055c68249da2fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_train['text'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_train['text']))\n",
    "df_train['text_encoded'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_train['text_encoded']))\n",
    "\n",
    "df_test['text'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_test['text']))\n",
    "df_test['text_encoded'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_test['text_encoded']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total =  77.30015683174133\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "glove = api.load(\"glove-twitter-50\") \n",
    "print(\"total = \",(time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fasttext Transforms start at: 2019-06-13 22:14:20.628054\n",
      "Number of Unique Test Tokens for Fasttext transform 57550\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb62fceacae45af9900f2069a4f43e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fasttext Transforms done at: 2019-06-13 22:14:22.367038\n",
      "Fasttext Transforms start at: 2019-06-13 22:14:22.372857\n",
      "Number of Unique Test Tokens for Fasttext transform 283052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ac8a1c78854fd481c8f502b4567e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2315162), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fasttext Transforms done at: 2019-06-13 22:14:47.888149\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ptr_glove_50 = PreTrainedEmbeddingsTransformer(glove,size=50)\n",
    "ptr_glove_50.fit()\n",
    "\n",
    "df_test['glove_encoded-50'] = ptr_glove_50.transform(df_test['text'].values)\n",
    "df_train['glove_encoded-50'] = ptr_glove_50.transform(df_train['text'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "fasttext = api.load(\"fasttext-wiki-news-subwords-300\") \n",
    "print(\"total = \",(time()-start))\n",
    "\n",
    "ptr = PreTrainedEmbeddingsTransformer(fasttext,size=300)\n",
    "ptr.fit()\n",
    "\n",
    "df_train['fasttext_encoded'] = ptr.transform(df_train['text'].values)\n",
    "df_test['fasttext_encoded'] = ptr.transform(df_test['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "text_max_features = 50000\n",
    "text_maxlen = 100\n",
    "text_embedding_dims = 50\n",
    "\n",
    "enc_maxlen = 100\n",
    "enc_embedding_dims = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "uniques, coded_id = np.unique(y, return_inverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "X_enc,X_text,y = df_train['glove_encoded-50'].values,df_train['text_encoded'].values,df_train['target'].values\n",
    "# X_enc = sequence.pad_sequences(X_enc, maxlen=enc_maxlen)\n",
    "X_text = sequence.pad_sequences(X_text, maxlen=text_maxlen)\n",
    "y = np_utils.to_categorical(coded_id, len(uniques))\n",
    "\n",
    "x_train_enc, x_test_enc,x_train_text, x_test_text, y_train, y_test = train_test_split(X_enc,X_text, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# x_train_enc = x_train_enc.reshape((-1,100,enc_embedding_dims,1))\n",
    "# x_test_enc = x_test_enc.reshape((-1,100,enc_embedding_dims,1))\n",
    "\n",
    "# datagen = ImageDataGenerator(featurewise_center=True,featurewise_std_normalization=True,)\n",
    "# datagen.fit(x_train_enc)\n",
    "# train_iterator = datagen.flow(x_train_enc, y_train, batch_size=len(x_train_enc),shuffle=True)\n",
    "# validation_iterator = datagen.flow(x_test_enc, y_test, batch_size=len(x_test_enc),shuffle=True)\n",
    "\n",
    "# x_train_enc = train_iterator.next()[0]\n",
    "# x_test_enc = validation_iterator.next()[0]\n",
    "\n",
    "# x_train_enc = x_train_enc.reshape((-1,100,enc_embedding_dims))\n",
    "# x_test_enc = x_test_enc.reshape((-1,100,enc_embedding_dims))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_joiner(intermediates):\n",
    "    x = concatenate(intermediates)\n",
    "    x = transition_layer(x, n_kernels=32,dropout=0)\n",
    "    x = conv_layer(x,n_kernels=len(uniques),kernel_size=3,padding='same',dropout=0,dilation_rate=2)\n",
    "    print(\"Before FC Intermediate =\",K.int_shape(x))\n",
    "    out = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_output_fcnn(inputs):\n",
    "    x = Activation(\"softmax\")(inputs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cnn(inputs):\n",
    "    x = inputs\n",
    "    filters = 128\n",
    "    kernel_size = 3\n",
    "    x = Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1)(x)\n",
    "\n",
    "    x1 = MaxPooling1D(pool_size=2)(x)\n",
    "    x2 = AveragePooling1D(pool_size=2)(x)\n",
    "    x = concatenate([x1,x2])\n",
    "\n",
    "    xp = x\n",
    "    x = transition_layer(x, n_kernels=32,dropout=0)\n",
    "    x = conv_layer(x, n_kernels=64,kernel_size=3,dilation_rate=2,padding='same')\n",
    "    x = concatenate([x,xp])\n",
    "    x = transition_layer(x, n_kernels=32,dropout=0)\n",
    "    x = conv_layer(x,n_kernels=len(uniques),kernel_size=3,dilation_rate=2,padding='same',dropout=0)\n",
    "    intermidate = x\n",
    "    # we use max pooling:\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    print(\"W1 Intermediate =\",K.int_shape(intermidate))\n",
    "    return intermidate, aux_output_fcnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_cnn():\n",
    "    main_input = Input(shape=(enc_maxlen,enc_embedding_dims), dtype='float32')\n",
    "    x = main_input\n",
    "    x = conv_layer(x,n_kernels=32,kernel_size=3,padding='valid')\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = conv_layer(x,n_kernels=len(uniques),kernel_size=3,dilation_rate=3,padding='same',dropout=0)\n",
    "    \n",
    "    \n",
    "\n",
    "    intermidate = x\n",
    "    print(\"Pretrained Intermediate =\",K.int_shape(intermidate))\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    return main_input,intermidate, aux_output_fcnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_layer(flattened):\n",
    "    \n",
    "    x = flattened\n",
    "    print(\"Final Layer incoming = \",K.int_shape(x))\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = Activation(\"softmax\")(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 Intermediate = (None, 49, 1316)\n",
      "Pretrained Intermediate = (None, 49, 1316)\n",
      "Before FC Intermediate = (None, 49, 1316)\n",
      "Final Layer incoming =  (None, 1316)\n",
      "Params =  1509020\n"
     ]
    }
   ],
   "source": [
    "word_input = Input(shape=(text_maxlen,), dtype='int32')\n",
    "x = Embedding(text_max_features,\n",
    "            text_embedding_dims,\n",
    "            input_length=text_maxlen)(word_input)\n",
    "\n",
    "w1_intermidate, w1_output = word_cnn(x)\n",
    "\n",
    "p_inputs,p_intermidate, p_output = pretrained_embedding_cnn()\n",
    "\n",
    "\n",
    "intermediates = [w1_intermidate,p_intermidate]\n",
    "x = intermediate_joiner(intermediates)\n",
    "\n",
    "op = final_layer(x)\n",
    "\n",
    "outputs = [w1_output,p_output,op]\n",
    "inputs = [word_input,p_inputs]\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = Adam(lr=0.001,)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['acc'],loss_weights=[0.2,0.2,1.0])\n",
    "\n",
    "print(\"Params = \",model.count_params())\n",
    "# 13331414\n",
    "# 3562108\n",
    "# 3227580"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 Intermediate = (None, 49, 1316)\n",
      "Params =  2659604\n"
     ]
    }
   ],
   "source": [
    "word_input = Input(shape=(text_maxlen,), dtype='int32')\n",
    "x = Embedding(text_max_features,\n",
    "            text_embedding_dims,\n",
    "            input_length=text_maxlen)(word_input)\n",
    "\n",
    "w1_intermidate, w1_output = word_cnn(x)\n",
    "\n",
    "\n",
    "outputs = [w1_output]\n",
    "inputs = [word_input]\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = Adam(lr=0.001,)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Params = \",model.count_params())\n",
    "# 13331414\n",
    "# 3562108\n",
    "# 3227580"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.get_value(model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00020000001"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1852129 samples, validate on 463033 samples\n",
      "Epoch 1/2\n",
      "1852129/1852129 [==============================] - 474s 256us/step - loss: 0.8816 - acc: 0.8014 - val_loss: 0.8111 - val_acc: 0.8172\n",
      "Epoch 2/2\n",
      "1852129/1852129 [==============================] - 474s 256us/step - loss: 0.8195 - acc: 0.8135 - val_loss: 0.8240 - val_acc: 0.8139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f46a0d67cf8>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_value(model.optimizer.lr, K.get_value(model.optimizer.lr)/5)\n",
    "K.get_value(model.optimizer.lr)\n",
    "model.fit([x_train_text], [y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=2,\n",
    "          validation_data=([x_test_text], [y_test]),\n",
    "         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 463033 samples, validate on 1852129 samples\n",
      "Epoch 1/1\n",
      "463033/463033 [==============================] - 242s 523us/step - loss: 0.8580 - acc: 0.8058 - val_loss: 0.7426 - val_acc: 0.8299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f46a08bed30>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_test_text], [y_test],\n",
    "          batch_size=batch_size,\n",
    "          epochs=1,\n",
    "          validation_data=([x_train_text], [y_train]),\n",
    "         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 100)\n",
      "(1852129, 100)\n"
     ]
    }
   ],
   "source": [
    "xt = df_test['text_encoded'].values\n",
    "xt = sequence.pad_sequences(xt, maxlen=text_maxlen)\n",
    "print(xt.shape)\n",
    "print(x_train_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "y_preds = model.predict(xt)\n",
    "\n",
    "print(len(y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = uniques[np.argmax(y_preds,1)]\n",
    "# uniques[y_code.argmax(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "result = df_test[[\"ID\"]]\n",
    "result['target'] = preds\n",
    "\n",
    "result.to_csv(\"results-4.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
