{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T10:56:06.982560Z",
     "start_time": "2019-06-12T10:56:06.943268Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import time\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np_utils\n",
    "%matplotlib inline\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, DepthwiseConv2D, Conv2D, SeparableConv2D, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers import Input, concatenate, LeakyReLU\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Nadam, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.regularizers import l2\n",
    "%config InlineBackend.figure_format='retina'\n",
    "from keras_contrib.callbacks import CyclicLR\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from data_science_utils.vision.keras import *\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import missingno as msno\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "from data_science_utils import dataframe as df_utils\n",
    "from data_science_utils import models as model_utils\n",
    "from data_science_utils import plots as plot_utils\n",
    "from data_science_utils.dataframe import column as column_utils\n",
    "from data_science_utils import misc as misc\n",
    "from data_science_utils import preprocessing as pp_utils\n",
    "from data_science_utils import nlp as nlp_utils\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from data_science_utils.dataframe import get_specific_cols\n",
    "\n",
    "import more_itertools\n",
    "from more_itertools import flatten\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lib' from '/home/ec2-user/SageMaker/ML_hackathon_2019/lib.py'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "from importlib import reload\n",
    "import lib\n",
    "reload(lib)\n",
    "from lib import *\n",
    "\n",
    "from oclr import OneCycleLR, LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T09:42:02.848654Z",
     "start_time": "2019-06-12T09:41:52.143642Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"asin_classification/train.csv\")\n",
    "df_test = pd.read_csv(\"asin_classification/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2315162, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(100000, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f354a8359044af9a800346a4e35e3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2315162), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7635e2ddf424f87bd9dae7cfaa418a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2315162), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5fb907501b4901997c604c1257153d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8aa48d120c437982528fb78ca32442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_train['text'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_train['text']))\n",
    "df_train['text_encoded'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_train['text_encoded']))\n",
    "\n",
    "df_test['text'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_test['text']))\n",
    "df_test['text_encoded'] = Parallel(n_jobs=16, backend=\"loky\")(delayed(ast.literal_eval)(x) for x in tqdm(df_test['text_encoded']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "glove = api.load(\"glove-twitter-50\") \n",
    "print(\"total = \",(time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fasttext Transforms start at: 2019-06-13 20:18:34.791308\n",
      "Number of Unique Test Tokens for Fasttext transform 57550\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2f258f7e7e435aa4e392b9554d3ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fasttext Transforms done at: 2019-06-13 20:18:39.466797\n",
      "Fasttext Transforms start at: 2019-06-13 20:18:39.512225\n",
      "Number of Unique Test Tokens for Fasttext transform 283052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18d22f98de14e49a4784aad66b7d94c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2315162), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fasttext Transforms done at: 2019-06-13 20:19:03.966135\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ptr_glove_50 = PreTrainedEmbeddingsTransformer(glove,size=50)\n",
    "ptr_glove_50.fit()\n",
    "\n",
    "df_test['glove_encoded-50'] = ptr_glove_50.transform(df_test['text'].values)\n",
    "df_train['glove_encoded-50'] = ptr_glove_50.transform(df_train['text'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "fasttext = api.load(\"fasttext-wiki-news-subwords-300\") \n",
    "print(\"total = \",(time()-start))\n",
    "\n",
    "ptr = PreTrainedEmbeddingsTransformer(fasttext,size=300)\n",
    "ptr.fit()\n",
    "\n",
    "df_train['fasttext_encoded'] = ptr.transform(df_train['text'].values)\n",
    "df_test['fasttext_encoded'] = ptr.transform(df_test['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "text_max_features = 50000\n",
    "text_maxlen = 100\n",
    "text_embedding_dims = 20\n",
    "\n",
    "enc_maxlen = 100\n",
    "enc_embedding_dims = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# uniques[np.argmax(y_code,1)]\n",
    "# uniques[y_code.argmax(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "X_enc,X_text,y = df_train['glove_encoded-50'].values,df_train['text_encoded'].values,df_train['target'].values\n",
    "X_enc = sequence.pad_sequences(X_enc, maxlen=enc_maxlen)\n",
    "X_text = sequence.pad_sequences(X_text, maxlen=text_maxlen)\n",
    "\n",
    "uniques, coded_id = np.unique(y, return_inverse=True)\n",
    "y = np_utils.to_categorical(coded_id, len(uniques))\n",
    "\n",
    "x_train_enc, x_test_enc,x_train_text, x_test_text, y_train, y_test = train_test_split(X_enc,X_text, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train_enc = x_train_enc.reshape((-1,100,enc_embedding_dims,1))\n",
    "x_test_enc = x_test_enc.reshape((-1,100,enc_embedding_dims,1))\n",
    "\n",
    "datagen = ImageDataGenerator(featurewise_center=True,featurewise_std_normalization=True,)\n",
    "datagen.fit(x_train_enc)\n",
    "train_iterator = datagen.flow(x_train_enc, y_train, batch_size=len(x_train_enc),shuffle=True)\n",
    "validation_iterator = datagen.flow(x_test_enc, y_test, batch_size=len(x_test_enc),shuffle=True)\n",
    "\n",
    "x_train_enc = train_iterator.next()[0]\n",
    "x_test_enc = validation_iterator.next()[0]\n",
    "\n",
    "x_train_enc = x_train_enc.reshape((-1,100,enc_embedding_dims))\n",
    "x_test_enc = x_test_enc.reshape((-1,100,enc_embedding_dims))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_joiner(intermediates):\n",
    "    x = concatenate(intermediates)\n",
    "    x = transition_layer(x, n_kernels=128,dropout=0)\n",
    "    x1 = conv_layer(x,n_kernels=32,kernel_size=3,padding='same',dilation_rate=2)\n",
    "    x2 = conv_layer(x,n_kernels=128,kernel_size=3,padding='same')\n",
    "    x = concatenate([x,x1,x2])\n",
    "    \n",
    "    x = transition_layer(x, n_kernels=64,dropout=0)\n",
    "    x = conv_layer(x,n_kernels=len(uniques),kernel_size=3,padding='same',dropout=0)\n",
    "    print(\"Before FC Intermediate =\",K.int_shape(x))\n",
    "    out = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_output_fcnn(inputs):\n",
    "    x = Activation(\"softmax\")(inputs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cnn(inputs):\n",
    "    x = inputs\n",
    "    filters = 128\n",
    "    kernel_size = 3\n",
    "    x = Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1)(x)\n",
    "\n",
    "    x1 = MaxPooling1D(pool_size=2)(x)\n",
    "    x2 = AveragePooling1D(pool_size=2)(x)\n",
    "    x = concatenate([x1,x2])\n",
    "\n",
    "    xp = x\n",
    "    x = transition_layer(x, n_kernels=64,dropout=0)\n",
    "    x = conv_layer(x, n_kernels=128,kernel_size=3,dilation_rate=2,padding='same')\n",
    "    x = concatenate([x,xp])\n",
    "    x = transition_layer(x, n_kernels=32,dropout=0)\n",
    "    x = conv_layer(x,n_kernels=len(uniques),kernel_size=3,dilation_rate=2,padding='same',dropout=0)\n",
    "    intermidate = x\n",
    "    # we use max pooling:\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    print(\"W1 Intermediate =\",K.int_shape(intermidate))\n",
    "    return intermidate, aux_output_fcnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_cnn():\n",
    "    main_input = Input(shape=(enc_maxlen,enc_embedding_dims), dtype='float32')\n",
    "    x = main_input\n",
    "    x = conv_layer(x,n_kernels=128,kernel_size=3,padding='valid')\n",
    "    \n",
    "    x1 = MaxPooling1D(pool_size=2)(x)\n",
    "    x2 = AveragePooling1D(pool_size=2)(x)\n",
    "    x = concatenate([x1,x2])\n",
    "    \n",
    "    xp = x\n",
    "    x = transition_layer(x, n_kernels=64,dropout=0)\n",
    "    x = conv_layer(x,n_kernels=128,kernel_size=3,padding='same')\n",
    "    x = transition_layer(x, n_kernels=64,dropout=0)\n",
    "\n",
    "\n",
    "    x = concatenate([x,xp])\n",
    "    x = transition_layer(x, n_kernels=32,dropout=0)\n",
    "    x = conv_layer(x,n_kernels=len(uniques),kernel_size=3,dilation_rate=2,padding='same',dropout=0)\n",
    "    \n",
    "    \n",
    "\n",
    "    intermidate = x\n",
    "    print(\"Pretrained Intermediate =\",K.int_shape(intermidate))\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    return main_input,intermidate, aux_output_fcnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_layer(flattened):\n",
    "    \n",
    "    x = flattened\n",
    "    print(\"Final Layer incoming = \",K.int_shape(x))\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = Activation(\"softmax\")(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 Intermediate = (None, 49, 1316)\n",
      "Pretrained Intermediate = (None, 49, 1316)\n",
      "Before FC Intermediate = (None, 49, 1316)\n",
      "Final Layer incoming =  (None, 1316)\n",
      "Params =  2086460\n"
     ]
    }
   ],
   "source": [
    "word_input = Input(shape=(text_maxlen,), dtype='int32')\n",
    "x = Embedding(text_max_features,\n",
    "            text_embedding_dims,\n",
    "            input_length=text_maxlen)(word_input)\n",
    "\n",
    "w1_intermidate, w1_output = word_cnn(x)\n",
    "\n",
    "p_inputs,p_intermidate, p_output = pretrained_embedding_cnn()\n",
    "\n",
    "\n",
    "intermediates = [w1_intermidate,p_intermidate]\n",
    "x = intermediate_joiner(intermediates)\n",
    "\n",
    "op = final_layer(x)\n",
    "\n",
    "outputs = [w1_output,p_output,op]\n",
    "inputs = [word_input,p_inputs]\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = Adam(lr=0.0001,)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['acc'],loss_weights=[0.2,0.2,1.0])\n",
    "\n",
    "print(\"Params = \",model.count_params())\n",
    "# 13331414\n",
    "# 3562108\n",
    "# 3227580"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, 100, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_127 (Conv1D)             (None, 98, 128)      19328       input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 98, 128)      512         conv1d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 98, 128)      0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_128 (Dropout)           (None, 98, 128)      0           activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 49, 128)      0           dropout_128[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_19 (AveragePo (None, 49, 128)      0           dropout_128[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_64 (Concatenate)    (None, 49, 256)      0           max_pooling1d_19[0][0]           \n",
      "                                                                 average_pooling1d_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 100, 20)      1000000     input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_128 (Conv1D)             (None, 49, 64)       16448       concatenate_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_122 (Conv1D)             (None, 98, 128)      7808        embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 49, 64)       256         conv1d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 49, 128)      0           conv1d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_18 (AveragePo (None, 49, 128)      0           conv1d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 49, 64)       0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_62 (Concatenate)    (None, 49, 256)      0           max_pooling1d_18[0][0]           \n",
      "                                                                 average_pooling1d_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_129 (Dropout)           (None, 49, 64)       0           activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_123 (Conv1D)             (None, 49, 64)       16448       concatenate_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_129 (Conv1D)             (None, 49, 128)      24704       dropout_129[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 49, 64)       256         conv1d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 49, 128)      512         conv1d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 49, 64)       0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 49, 128)      0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_124 (Dropout)           (None, 49, 64)       0           activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_130 (Dropout)           (None, 49, 128)      0           activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_124 (Conv1D)             (None, 49, 128)      24704       dropout_124[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_130 (Conv1D)             (None, 49, 64)       8256        dropout_130[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 49, 128)      512         conv1d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 49, 64)       256         conv1d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 49, 128)      0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 49, 64)       0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_125 (Dropout)           (None, 49, 128)      0           activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_131 (Dropout)           (None, 49, 64)       0           activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_63 (Concatenate)    (None, 49, 384)      0           dropout_125[0][0]                \n",
      "                                                                 concatenate_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_65 (Concatenate)    (None, 49, 320)      0           dropout_131[0][0]                \n",
      "                                                                 concatenate_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_125 (Conv1D)             (None, 49, 32)       12320       concatenate_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_131 (Conv1D)             (None, 49, 32)       10272       concatenate_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 49, 32)       128         conv1d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 49, 32)       128         conv1d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 49, 32)       0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 49, 32)       0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_126 (Dropout)           (None, 49, 32)       0           activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_132 (Dropout)           (None, 49, 32)       0           activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_126 (Conv1D)             (None, 49, 1316)     127652      dropout_126[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_132 (Conv1D)             (None, 49, 1316)     127652      dropout_132[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 49, 1316)     5264        conv1d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 49, 1316)     5264        conv1d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 49, 1316)     0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 49, 1316)     0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_127 (Dropout)           (None, 49, 1316)     0           activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_133 (Dropout)           (None, 49, 1316)     0           activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_66 (Concatenate)    (None, 49, 2632)     0           dropout_127[0][0]                \n",
      "                                                                 dropout_133[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_133 (Conv1D)             (None, 49, 128)      337024      concatenate_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 49, 128)      512         conv1d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 49, 128)      0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_134 (Dropout)           (None, 49, 128)      0           activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_134 (Conv1D)             (None, 49, 32)       12320       dropout_134[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_135 (Conv1D)             (None, 49, 128)      49280       dropout_134[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 49, 32)       128         conv1d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 49, 128)      512         conv1d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 49, 32)       0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 49, 128)      0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_135 (Dropout)           (None, 49, 32)       0           activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_136 (Dropout)           (None, 49, 128)      0           activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_67 (Concatenate)    (None, 49, 288)      0           dropout_134[0][0]                \n",
      "                                                                 dropout_135[0][0]                \n",
      "                                                                 dropout_136[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_136 (Conv1D)             (None, 49, 64)       18496       concatenate_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 49, 64)       256         conv1d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 49, 64)       0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_137 (Dropout)           (None, 49, 64)       0           activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_137 (Conv1D)             (None, 49, 1316)     253988      dropout_137[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 49, 1316)     5264        conv1d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 49, 1316)     0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_138 (Dropout)           (None, 49, 1316)     0           activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 1316)         0           dropout_127[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 1316)         0           dropout_133[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 1316)         0           dropout_138[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 1316)         0           global_max_pooling1d_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 1316)         0           global_average_pooling1d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 1316)         0           global_average_pooling1d_20[0][0]\n",
      "==================================================================================================\n",
      "Total params: 2,086,460\n",
      "Trainable params: 2,076,580\n",
      "Non-trainable params: 9,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_model = multi_gpu_model(model, gpus=8)\n",
    "parallel_model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=optimizer,\n",
    "              metrics=['acc'],loss_weights=[0.2,0.2,1.0])\n",
    "\n",
    "parallel_model.fit([x_train_text,x_train_enc], [y_train,y_train,y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=([x_test_text,x_test_enc], [y_test,y_test,y_test]),\n",
    "         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 1159686 samples, validate on 289922 samples\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "model.fit([x_train_text,x_train_enc], [y_train,y_train,y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=([x_test_text,x_test_enc], [y_test,y_test,y_test]),\n",
    "         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
